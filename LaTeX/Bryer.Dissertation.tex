%\documentclass[letterpaper,12pt]{article} %For final submission
\documentclass[letterpaper,12p,twoside]{article} %For two-sided printing

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage[tocbib,bibnewpage]{apacite}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{color}
\definecolor{lightgrey}{rgb}{.5,.5,.5}
\usepackage{tabularx}
\usepackage{appendix}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{MnSymbol}
\usepackage{moreverb}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{lscape}

% Sets the amount of padding below a figure
\setlength{\belowcaptionskip}{0pt}
\setlength{\floatsep}{0pt}

\usepackage{myapa}
\FiveLevelHeading
\raggedright
\setlength{\parindent}{0.3in}

\usepackage{setspace}
\doublespacing

\usepackage{ifthen}
\newboolean{draft}
\setboolean{draft}{false} %===== DRAFT =====%
\ifthenelse{\boolean{draft}}{
	\usepackage{draftwatermark}
	\SetWatermarkScale{6}
}{}

\def\quote{\singlespacing\parindent2em\hangindent2em}

\newcommand{\smaller}{\fontsize{9}{10}\selectfont}

\renewcommand{\normalsize}{\fontsize{12}{13}\selectfont}

\oddsidemargin 0.5in
%\evensidemargin 0.5in %For final submission
\evensidemargin 0in %For two-sided printing
\textwidth 6.0in
\headheight 0.0in
\topmargin 0.0in
\textheight 8.5in
\doublerulesep 0pt

\newcommand{\thickline}{\hline\hline\hline}

\pagenumbering{roman}
\renewcommand*\contentsname{Table of Contents}

\begin{document}
\begin{titlepage}
\vspace*{\fill}
\begin{center}
A NATIONAL STUDY COMPARING\\
CHARTER AND TRADITIONAL PUBLIC SCHOOLS\\
USING PROPENSITY SCORE ANALYSIS\ \\\ \\
by
\ \\ \ \\
Jason M. Bryer\\
\ \\ \ \\ \ \\
A Dissertation Submitted to the\\
University at Albany, State University of New York\\
In Partial Fulfillment of\\
the Requirements for the Degree of\\
Doctor of Philosophy\\
\ \\\ \\\ \\
School of Education\\
Department of Educational and Counseling Psychology\\
Division of Educational Psychology \& Methodology\\
2014
\ifthenelse{\boolean{draft}}{
	\ \\	\LARGE{}Draft as of \today
}{}
\end{center}
\vspace*{\fill}
\end{titlepage}


\clearpage
\begin{titlepage}
%%%%% Copyright page
\vspace*{\fill}
\begin{center}
A National Study Comparing\\
Charter and Traditional Public Schools\\
using Propensity Score Analysis
\ \\ \ \\ \ \\
by
\ \\ \ \\ \ \\
Jason M. Bryer
\ \\ \ \\ \ \\\ \\ \ \\ \ \\\ \\ \ \\ \ \\
COPYRIGHT 2014
\end{center}
\vspace*{\fill}
\end{titlepage}

\setkeys{Gin}{width=\textwidth} %Make images fit full text width of the page

\setcounter{page}{3}

%\cleardoublepage
\topskip0pt
\vspace*{2.5 in}
\begin{center}
\noindent To my wife, Heather,\\and three sons Gabriel, Miles, and Rowan\\for providing the inspiration and support\\for life's journey.
\end{center}
\vspace*{\fill}

%\setcounter{page}{3}

\cleardoublepage
\section{Abstract}

Unlike their private school counterparts, charter schools receive public funding but are relieved of some of the bureaucratic and regulatory constraints of public schools in exchange for being held accountable for student performance. Studies provide mixed results with regard to charter school performance. Charter schools are, by definition, schools of choice, and this means that observational data methods are required for comparing such schools with others. In observational data contexts, simple comparisons of two groups such as traditional public and charter schools typically ignore the inherent and systematic differences between the two groups. %citation?
However, given well-designed observational studies and appropriate analysis methods, the effects of the selection bias can be reduced, if not eliminated. The result is that the usual simple comparisons of two independent groups are replaced by comparisons that make adjustments for covariate differences. This study includes development of new methods, largely graphic in form, designed for observational data to compare two groups. These methods are then used to investigate the question of whether students who attend charter schools perform differently than their traditional public school counterparts on two key academic domains: reading and mathematics. The new methods represent extensions of propensity score analysis \cite{RosenbaumRubin1983} by aiding descriptions and aim in reducing selection bias in the context of clustered data.

Using data from the 2009 National Assessment of Educational Progress (NAEP) for mathematics and reading at grades four and eight, estimates of the differences between charter and traditional public schools were calculated at the state and national levels. This study finds that there is wide variability in math and reading performance for charter schools. But in aggregate, charter schools do not perform any differently than their traditional public school counterparts. The new methods are also used to examine the relationship between the quality of state charter laws as determined by the National Alliance for Public Charter Schools \citeyear<NAPCS;>{NAPCS2010a} and differences in NAEP scores are explored. Results suggest that there is small to moderate correlation between NAPCS's ratings of charter law quality and differences in NAEP scores within each state.

\cleardoublepage 
\section{Acknowledgements} 

\ \\
\begin{center}
\textit{It takes a village to...\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ to complete a dissertation.}
\end{center}
\ \\\ \\

Being a graduate student is very much like being a child in a village. You enter barely speaking the language, and with the help of an advisor, faculty members, and fellow students, you begin to develop an understanding of the village in which you live. The dissertation is the culmination of all the learning within that village and marks a transition to life outside the village. Like the original proverb states, \textit{it takes a village to raise a child}, a dissertation, although the result of an individual endeavor, is impossible without the support of many people. There are too many people who have influenced and taught me so much over the years to name them all. But some have had a substantial influence on my growth, and I wish to thank them here.

To my advisor and chair, Bob Pruzek, who introduced me to new ways of seeing and measuring the world. By introducing me to PSA, graphics, and R, you have significantly changed my professional life. You have provided the inspiration and foundation not only for the work in this dissertation, but for the work I will be doing for years to come.

To my entire committee, Bruce Dudek, Heidi Andrade, and Katy Schiller. But especially to Bruce and Heidi who went above and beyond to help me bring this dissertation to completion. Their time, patience, and insight are so greatly appreciated. Also a very special thanks to Joan Newman who coordinated and provided invaluable advice in bringing this dissertation to completion.

To my wife, Heather. She has been a constant supporter and cheerleader providing the encouragement when I needed it most.  And for being such a wonderful editor, making sure everything I write sounded the best it could.

To Gabriel, Miles, and Rowan, who provide the inspiration to continue to be the best father, teacher, and person I can be.

To my mom, who has been a constant believer in me and for being a great Nana for the boys when dad was teaching or writing.

To my Excelsior College family, Lisa Daniels, Patti Croop, Jane Weyers, Bethany de Barros, Kim Speershneider, Vaishali Jahagirdar, and Erin Applegarth Vitali, for providing invaluable time, support, and encouragement. And especially to the R Groupers!

To the National Center for Education Statistics (NCES) and Education Testing Service (ETS) who provided training on the use of NAEP and financial support for the development of the \texttt{naep} R package.

To the fourth and eighth grades students who participated in NAEP. Your participation helps make education better for you and all students.

\cleardoublepage

\addtocontents{lot}{\vskip 12pt}
\addtocontents{lof}{\vskip 12pt}
\addcontentsline{toc}{section}{Table of Contents}
\setcounter{tocdepth}{5}
\tableofcontents
\clearpage
\addcontentsline{toc}{section}{List of Tables}  \listoftables
\clearpage
\addcontentsline{toc}{section}{List of Figures} \listoffigures


%==================== CHAPTER 1 ====================================================================
\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Chapter 1: Introduction}

Since the opening of the first charter school in Minnesota in 1991, the United States\footnote{Though this study focuses on charter schools in the U.S., Canada \cite{canada2007}, Chile \cite{larranaga2004}, England \cite{WohlstetterAnderson1994}, Germany \cite{herbst2006}, and New Zealand \cite{lander2001} also have charter schools.} has increasingly embraced charter schools as an important option for educational reform. In the last 10 years alone, the number of charter schools has grown from 507 in the 1998-1999 school year to 6,187 in the 2012-2013 school year \cite<see Figure \ref{fig:charterSchoolGrowth};>{cernumbers}. By the 2011-12 school year, 40 states and the District of Columbia have charter school laws (see \hyperref[appendixA]{Appendix A} for enrollment by state). Given Arne Duncan's appointment as Secretary of Education by President Barack Obama and the Race to the Top program (specifically the requirement that states raise the limit on the number of charter schools in their state to be eligible), charter school growth and support is unlikely to slow in the near future. Moreover, some charter school supporters argue for the eventual replacement of traditional public schools with charter schools \cite{Ravitch2013,Ravitch2013LATimes}.

In principle, charter schools opt out of some bureaucratic rules and union contracts in exchange for the academic autonomy needed to create better academic environments for students \cite{wells2002}. The idea is that  teachers, administrators, students, and the community that comprise the charter school would be free to innovate. That is, charter schools serve as experimental schools, and their innovations would inform the reform of public education at large.

\begin{figure}[tp]
\includegraphics[width=\textwidth]{../Figures/CharterSchoolGrowth.pdf}
\caption{Charter school growth 1999-2013}
\label{fig:charterSchoolGrowth}
\end{figure}

%Clearly charter schools have become a popular vehicle for educational reform among parents as well. The \citeA{cersurvey} reports that 59\% of charter schools have waiting lists averaging 198 students. Charter schools provide an apparent choice to parents and are copacetic to the United State's individualistic culture \cite{hofstede2004,maccall1847,swart1962}. Moreover, like so many other fields, school reform has further emphasized marketization and privatization \cite{wells2002}. The influence of capitalism on education is not new. A major contributor to the expanded role of education during the industrial revolution is capitalism itself. That is, education expanded its initial purpose of providing a minimally informed electorate to providing an educated work force, not to mention keeping children off the streets as child labor laws came into existence. However, the shift of capitalistic principles from being the inspiration of educational reform to being the educational reform has profound implications.

%Proponents of charter schools argue that public schools have been bogged down by bureaucracy and union contracts. Freeing schools of these requirements then allows teachers and schools to innovate, which in theory leads to increased student performance. The principled argument is the ``market metaphor" \cite{wells2002}. That is, if schools were forced to compete for ``customers" (i.e. students), then the differentiating factor between schools would be their quality of education. 

%Opponents on the other hand have questioned the accountability, equity, effectiveness, and sustainability of charter schools. Several studies have shown that charter schools are not only failing to increase student performance, in many instances they are performing well under their traditional public school counterparts \cite<see e.g.,>{credo,BraunJenkinsGrigg2006,aft2004}. Still, others argue whether charter schools may be a solution in search of a problem. \citeA{carnoy2005} in summarizing the controversy that ensued after the \citeA{aft2004} study argue that:


Charter schools have become a popular vehicle for educational reform among parents. The \citeA{cersurvey} reports that 59\% of charter schools have waiting lists averaging 198 students. Charter schools provide an apparent choice to parents and are in line with the individualistic culture of the United States \cite{hofstede2004,maccall1847,swart1962}. 

Like so many other fields, school reform has further emphasized marketization and privatization \cite{wells2002}. The influence of capitalism on education is not new. A major contributor to the expanded role of education during the industrial revolution is capitalism itself. That is, education expanded its initial purpose of providing a minimally informed electorate to providing an educated work force, not to mention keeping children off the streets as child labor laws came into existence. However, the shift of capitalistic principles from being the inspiration of educational reform to being the educational reform has profound implications. Proponents of charter schools argue that public schools have been bogged down by bureaucracy and union contracts. Freeing schools of these requirements then allows teachers and schools to innovate which, in theory, leads to increased student performance. The principled argument is the ``market metaphor" \cite{wells2002}. That is, if schools were forced to compete for ``customers" (i.e. students and parents), then the differentiating factor between schools would be their quality of education. 

Opponents, on the other hand, have questioned the accountability, equity, effectiveness, and sustainability of charter schools. Several studies have shown that charter schools are not only failing to increase student performance, but in many instances are performing well below their traditional public school counterparts \cite<see e.g.,>{credo,BraunJenkinsGrigg2006,aft2004}. 

Others argue that charter schools may be a solution in search of a problem. \citeA{carnoy2005}, in summarizing the controversy that ensued after the \citeA{aft2004} study argue that:

\begin{quote} \normalsize
If, however, charter schools are not improving the achievement of disadvantaged children, it may be that the cause of low student performance is not bureaucratic rules but something else. When a treatment is based on a diagnosis, and the treatment doesn't work, it is prudent to examine not only whether the treatment should be improved, but also whether the diagnosis might be flawed. \cite{carnoy2005}
\end{quote}

\noindent This study primarily focuses on evaluating the effects of charter schools, or the treatment from Carnoy et al's point-of-view. However, as a result of the new methods developed, the relationship between the quality of state charter laws and differences between charter and traditional schools are explored.


\subsection{Statement of the Problem}

As \citeA{BettsHill2006} point out, there are three major obstacles to addressing the question of ``whether students in charter schools are learning more or less than they would have learned in conventional schools" (p. 1), namely:

\begin{enumerate}
\item The issues of counterfactuals. That is, there are several barriers to determine the causal relationship between school choice and learning, most significantly the fact that students and families self-select to attend charter schools.
\item The variation in types of charter schools.
\item The nature of student achievement. Research has shown there are many other factors that contribute to student success including, but not limited to, socioeconomic status, parents' education, student motivation, etc. \end{enumerate}

\noindent Deciphering how school choice contributes to student learning in the context of all the other factors proves difficult. Though these issues are significant, they can be reasonably addressed. I do not claim to fully account for these issues, but I attempt to address them using the best data and methods available while clearly stating the limitations.

Issue one is dealt with in detail in chapter three. However, in short, the propensity score analysis \cite<PSA;>{RosenbaumRubin1983} used for this study is, assuming proper implementation, the best approach to estimating causal inferences short of well  designed randomized experiments. Of course in the context of an observational study the fundamental problem of causal inference remains\footnote{The fundamental problem of causal inference states that it is impossible to observe the effect of a treatment and lack of treatment (usually referred to the control but is true for any two conditions) for any one subject simultaneously.} \cite{Holland1986}, but limitations of this are addressed.

%The issue of charter school variation is often cited in critiques of national or large scale charter school studies. Given that the charter school debate is a national debate with implications at the Federal level as exemplified by the \textit{No Child Left Behind} legislation of the George W. Bush Administration and the \textit{Race to the Top} policy of the Barak Obama Adminsitration, large scales studies are not only necessary, they are critical. If charter schools are to be offered as an alternative to traditional public schools, then charter schools as a whole must be evaluated against public schools as a whole. More specifically, it is important not to evaluate whether a particular charter school, or type of charter school, is better, but whether the entire charter school concept is a better approach for educational reform.

The issue of charter school variation is often cited in critiques of national or large scale charter school studies \cite<c.f.>{NAPCS2010}. However, given that the charter school debate is a national debate with implications at the federal level, as exemplified by the \textit{No Child Left Behind} legislation of the George W. Bush Administration and the \textit{Race to the Top} policy of the Barak Obama Administration, larger scale studies are necessary. Additional research on the effects of variation in types of charter schools is also needed but is beyond the scope of this study \cite<see also>{BettsTang2011}.

Third, the environmental, social, community, and cultural factors that contribute to a student's academic achievement are often significantly underestimated. Often educational reform, as exemplified by the \textit{No Child Left Behind Act} and \textit{Race to the Top}, places the responsibility solely on the school without consideration of the context in which the school operates. PSA addresses this issue by providing a method to adjust for observed characteristics by finding matched pairs or clusters. As such, comparisons between matched and clustered students substantially reduces the effects of these environmental factors providing a much better estimate of the effects of charter versus traditional public schools. 

A fourth issue not mentioned by \citeA{BettsHill2006} but often cited as a factor related to charter school performance are state laws, which vary widely. NAPCS annually publishes scores and rankings of the quality of state charter school laws \cite{NAPCS2010a,NAPCS2012}. These scores and rankings enable an exploratory analysis of the relationship between state laws and charter school effectiveness. Since one of the purposes of charter schools is to experiment with schools that operate outside existing bureaucratic rules and laws for public schools, exploring the relationship between the new rules and laws charter school operate under and student performance is important. Although this study cannot address the causal question of whether ``better" charter laws, as defined by the NAPCS, effect better student performance, this is the first known study to explore this relationship.

\subsection{Purpose of the Study}

A primary purpose of this study is to explore the differences in performance between charter and traditional public schools controlling for self-selection and observed characteristics. In addition, differences between states are examined in terms of the quality of state charter laws, as measured by The National Alliance for Public Charter Schools \citeyear<NAPCS>{NAPCS2010a}.

A secondary purpose is the development of a new set of methods for propensity score analysis with multilevel, or clustered, data. An additional aim of the study is to show how graphics can be used to address research questions in the context of multilevel propensity score analysis; another is to describe and illustrate the key features of a new package of R functions to facilitate multilevel propensity score analyses, vis-\`{a}-vis the \texttt{multilevelPSA} package in R. These new multilevel methods for propensity score analysis are presented within the context of more traditional methods for propensity score analysis, namely stratification and matching.

The newly developed \texttt{multilevelPSA} package is shown to provide an effective means of estimating and visualizing propensity score results with clustered (multilevel) data. These procedures are discussed more fully in chapters three and four. The use of pre-existing visualization procedures such as loess regression plots, density plots, as well as the PSA balance and assessment plots introduced by \citeA{HelmreichPruzek2009}, can provide critical insight into the analysis and eventual interpretation of results. More succinctly, the presentation of graphics in this study is not merely provided for diagnostic or descriptive purposes, but is a critical component of presenting, analyzing, and interpreting results.

This study compares the academic performance of charter and traditional public schools in two domains using the National Assessment of Educational Progress (NAEP), using multiple propensity score methods. More specifically, this study proposes to address three questions:

\begin{enumerate} 
	\item Given appropriate adjustments based on available student data, is there a discernible difference between charter and traditional public schools with regard to NAEP math and reading scores at grades 4 and 8?
	\item If so, what is the nature and magnitude of this difference for the two outcomes, reading and mathematics scores?
	\item What is the relationship, if any, of the quality of charter school laws to charter school student performance in NAEP math and reading scores at grades 4 and 8?
\end{enumerate}



%==================== CHAPTER 2 ====================================================================
\cleardoublepage
\section{Chapter 2: Review of the Literature}

Though Professor Ray Budde is often credited with the current charter school movement \cite{Kolderie2005}, the term \textit{school choice} can be traced back to Adam Smith's \textit{Wealth of Nations}, Thomas Paine's \textit{Rights of Man}, and John Stuart Mill's \textit{On Liberty} \cite{herbst2006}. Prior to the Revolutionary War, given the religious diversity of colonial America, issues of education were left to local communities. However, after the war, Revolutionary leaders argued that local schools were no longer sufficient for educating students for the emerging state and federal governments. It was Thomas Jefferson who, in 1779, introduced the first bill in Virginia that would establish a public school system. It was Jefferson, along with numerous other American intellects during the 1780s and 1790s, who was responsible for establishing public schools throughout the young nation, thereby relegating school choice to a choice between public schools and predominately religious private schools.

In the wake of the landmark report \textit{A Nation at Risk} \cite{nationatrisk}, \citeA{Budde1988} authored a pivotal document that started the charter school movement in the United States. In this document, Budde argues that system-wide changes to the way schools are structured are required, including: more rigorous curriculum and graduation standards; extended school days and year; more homework; teacher accountability for student results; termination of ``incompetent" teachers; and higher pay for teachers. To achieve these goals, he proposed a fundamental change to the ``internal organization of the school district... making substantial changes in the roles of teachers, principals, the superintendent, the school board, parents, and others in the community" (p. 16). More specifically, a framework for charter schools was proposed that includes five stages over a three year period (see Figure \ref{fig:timeline}). The five stages include: (1) generating ideas; (2) planning the charter; (3) preparing for teaching; (4) teaching under the educational charter; and (5) program monitoring and evaluation. For the first iteration of the cycle, stages one, two, and three occur prior to the opening of the school with stage one ideally beginning a full school year before. 

There are several features of this framework that deviate from traditional public school models, most notably the repetition of what may appear to be preparatory stages. That is, the charter school personnel must re-plan their school structure periodically (every three to five years according to Budde's framework) in a manner consistent with the initial charter school creation, thereby forcing a re-evaluation of the school bureaucracy.

\begin{figure}[tp]
\includegraphics[width=\textwidth]{../Figures/Timeline.pdf}
\caption[Stages of a charter school life cycle]{Stages of a charter school life cycle (adapted from Budde, 1988)}
\label{fig:timeline}
\end{figure}

Following the suggestions of Budde, Minnesota passed the first charter school law in 1991 with California following in 1992. As of spring 2009, 40 states and the District of Columbia had charter school laws which comprise 1,407,421 students in 4,578 schools \cite{cernumbers}. 


\subsection{Empirical Evidence for Charter School Effectiveness}

According to \citeA{whatweknow}, there are currently over 200 studies that examine charter school achievement. The \citeA{whatweknow} provides a review of 140 studies selected on several criteria. Their review reveals significant gaps in the research with regard to states evaluated, research quality that addresses achievement, as well as timeliness of results. This is further exemplified by a meta-analysis conducted by \citeA{BettsTang2008} that includes just 13 studies that represent nine states. In this section I provide an overview of the current literature available vis-\`{a}-vis the published meta-analysis and literature reviews. I then focus on two recent studies that together provide the context for the study proposed here: a hierarchical linear modeling analysis of the 2005 NAEP study \cite{BraunJenkinsGrigg2006} and a matching study comparing charter and public schools in 2009 in 16 states \cite{credo} and again in 2013 in 27 states \cite{credo2013}.


\subsubsection{Overview of Current Studies}

Research has shown that parents of students in charter schools are generally more satisfied with the charter school than the public school and also tend to be more involved in their child's education \cite{TeskeSchneider2001,VanourekMannoFinnBierlein1998}. However, their satisfaction may simply be a rationalization \cite{HubbardKulkarni2009}. Moreover, Fuller et al. \cite<1996, as cited in>{HubbardKulkarni2009} suggests that parents that choose charter schools ``believe that the charter must therefore be superior to a conventional public school" (p. 177). This is reinforced by a study conducted by \citeA{CullenJacobLevitt2005} that examines school choice in Chicago Public Schools, where more than half of the students elect to attend a public school (e.g. career academy, high-achieving school) other than their locally assigned school. Although students who opt out of their local school are more likely to graduate, \citeA{CullenJacobLevitt2005} argue that ``those who opt out are superior along unobservable dimensions such as their motivation level and parental involvement" (p. 755). 

The \citeauthor{whatweknow} \citeyear<NAPCS;>{whatweknow} provides perhaps the most comprehensive review of available research on charter school performance. The current report, \textit{Charter School Achievement: What We Know} is now in its fifth edition, having been updated periodically to account for recent studies. In addition to covering published research reports, the review includes unpublished reports such as conference presentations, dissertations, policy group and think tank reports, and state evaluations. Of the 210 studies identified in the fifth edition, 140 are included in their review. These 140 studies were determined to compare charter schools with traditional public schools, use ``serious research methods" (p. 2), and examine ``a significant segment of the charter sector" (p. x). The studies are then further classified into one of three categories: (1) panel studies that are longitudinal and examine student growth over time; (2) cohort change studies that are longitudinal but use some method other than tracking individual students; and (3) snapshot studies that examine school performance at a single point in time (also known as observational studies).

% Should start with the characterization, then say Table 1 summarizes...

Table \ref{charterAchievement} summarizes the findings of the 140 studies included, first by breaking out the year(s) upon which the study's data is based, and second by the results reported. It should be noted that many of the pre-2001 studies were concentrated in a few states (Arizona, California, Florida, North Carolina, and Texas). This is expected, given that these states were among the earliest to adopt charter school laws (see \hyperref[appendixA]{Appendix A}) as well as the substantial increase in the number of charter schools since 2000 (see Figure \ref{fig:charterSchoolGrowth}). The National Alliance of Public Charter Schools concluded that:

\begin{quote} \normalsize
[I]t becomes dramatically clear that studies examining public charter schools in more recent academic years show that charter schools produce more instances of larger achievement gains in both math and reading when compared to traditional public schools. (p. 3)
\end{quote}

\noindent However, this interpretation downplays the fact that approximately 30\% of charter schools performed worse than their traditional public school counterpart. These results are consistent with a recent study by the \citeA{credo} that reported that 37\% of charter schools performed worse than their public school counterpart in 2009 and 31\% in 2013.

\input{../Tables/Chapter2.table.SummaryCharterSchoolAcievement.tex}

\citeA{BettsTang2008} employ more stringent selection criteria for including studies in their meta-analysis. Only studies that used experimental, student-level, growth-based methods were included, resulting in a total of 14 studies published between 2001 and 2007 utilizing data ranging from 1998 through 2005. Similar to the review produced by the \citeA{whatweknow}, the studies were done in a limited number of locations, including Arizona, California (three from San Diego), Chicago, Delaware, Florida, Idaho, North Carolina, and Texas, with one anonymous location. Overall, the analysis of the available studies provide very mixed results. However, some patterns emerge, specifically that charter schools generally outperform traditional public schools in elementary school reading and middle school math, although effect sizes for the latter are small. Charter schools are generally underperforming traditional public schools in high school reading and math, but studies examining these grade levels are relatively small studies \cite<see also,>{whatweknow} and in these studies the effect sizes are also small.

% TODO: Brief summary paragraph, e.g., I hate to admit it but, in general, large-scale studies and reviews show that charter schools aren't all bad afterall… or something like that, followed by, however, many of the studies are limited by inappropriate statistical analyses [this latter part is important, as it begins to set up the need for your study, which is the main purpose of a lit review].

\subsection{National Charter School Studies}

There are two known studies that examine the differences between charter and traditional public schools from a national perspective. \citeA{BraunJenkinsGrigg2006} explored the differences using hierarchical linear modeling (HLM) with the 2005 National Assessment of Educational Progress (NAEP). The \citeA{credo,credo2013}, using their own data, used methods similar to propensity score matching. These studies are discussed in detail in the following sections. They provide a foundation for this study. Specifically, this study utilizes the high quality NAEP data used by \citeA{BraunJenkinsGrigg2006} with the methods similar to, but expanded upon, those used by the \citeA{credo,credo2013}.

\subsubsection{Comparing Charter and Public School Students' Using HLM and NAEP}

Braun, Jenkins, and Grigg have published two research reports utilizing NAEP data and HLM analyses that look at how public school students' test scores compare to those of private school students \citeyear{BraunJenkinsGrigg2006private} and charter schools students \citeyear{BraunJenkinsGrigg2006}. HLM was used because ordinary least squares or ANOVA is inappropriate since these statistical models do not account for the school effects. HLM provides a model with which school effects can be partitioned from student effects, thereby providing adjustments for the lack of independence of observations \cite<see e.g.,>{BrykRaudenbush1992,GelmanHill2006}. 

%\paragraph{Comparing private and public school students' NAEP scores} This study is briefly discussed because it served as precursor to the 2006 study comparing charter and public schools. The use of NAEP and methods are identical. For the private school study \cite{BraunJenkinsGrigg2006private}, results suggest that students in private schools scored significantly higher than public school students in both mathematics and reading at grades 4 and 8. This study served as a precursor of their 2006 study comparing charter and traditional public schools and provided HLM as an approach for comparing two types of schools. With regard to private and public schools, Braun \textit{et al} found differences that ranged from 8 points for grade 4 mathematics to 18 points for grade 8 reading. Adjusting for student characteristics such as gender, ethnicity, and proxies for social economic status with HLM resulted in reductions in all four comparisons of approximately 11 to 14 points. After adjustment, private school students still scored significantly higher than public school students in grade 8 reading, but public schools scored significantly better in grade 4 mathematics. There was no significant difference for grade 4 reading and grade 8 mathematics.

For the charter school study \cite{BraunJenkinsGrigg2006}, the analysis was conducted in three phases for both reading and mathematics. In phase one, all sampled charter schools were compared to all sampled public schools. Results suggest that, when student characteristics were taken into account, charter schools performed, on average, 4.2 points lower than public schools in reading (effect size is 0.11) and 4.7 points lower in mathematics (effect size is 0.17).

Phase two separated charter schools into two groups: charter schools associated with a public school district (PSD) and those that were not. The purpose for this analysis is to examine the relationship between two approaches to charter school governance and student achievement. One approach has the charter schools governed by the public school district in which they operate and those that are not. For reading, there was no significant difference between charter schools affiliated with a PSD and public schools. However, for schools not affiliated with a PSD, charter school students scored significantly lower than public school students, with an adjusted difference of 0.17 standard deviations. For mathematics, there was no difference between charter schools affiliated with a PSD and public schools, but charter schools not affiliated with PSD scored significantly lower, with an adjusted difference of 0.23 standard deviations.

Phase three compared only charter and publics schools located in a central city and serving a high-minority population. For reading, there was no significant difference between charter and public schools for any model. For mathematics, however, charter schools not affiliated with a PSD scored significantly lower than public school students with an adjusted difference of 0.17 standard deviations. There was no difference for schools affiliated with a PSD. 


%\clearpage

\subsubsection{The CREDO Study}

The \citeA{credo,credo2013} conducted a study of more than 1.7 million records from 2400 charter schools within 16 and 27 states in 2009 and 2013, respectively. The methodology involves creating a Virtual Control Record (VCR) for each charter school student \cite<see also,>{AbadieDiamondHainueller2007,nea} which is used to find matching students from an eligible traditional public school. Students within a traditional public school become available in a pool of potential matches when at least one student is identified as transferring to a charter school. Once the ``feeder schools" are identified, all students from feeder schools are pooled and serve as the source to select matches to the charter school students. Students are then matched on the following factors: grade-level, gender\footnote{Gender was not available in Florida}, race/ethnicity, free or reduced price lunch status, English language learner status, special education status, and prior test score on state achievement tests. This procedure, which is similar to propensity score matching, resulted in 83.7\% and 84.4\% of charter school students being matched to a public school student for reading and math, respectively.

Once matches were determined, ordinary least squares regression was utilized to analyze both math and reading scores of the charter school students and matched public school students separately. Moreover, controls for student characteristics, excluding gender, along with state indicators and scores affected by Hurricane Katrina, were added to the basic model. Overall results showed that charter school students performed, on average, 0.01 and 0.03 standard deviations below traditional public school students for reading and math, respectively. Both results are significant at \textit{p} $\leq$ 0.01.

Further analysis by \citeA{credo} revealed that the effectiveness of charter schools varied considerably by state. Five states (Arkansas, Colorado, Illinois, Louisiana, and Missouri) were found to have higher learning gains for charter schools. Six states (Arizona, Florida, Minnesota, New Mexico, Ohio, and Texas) were found to have lower learning gains for charter schools. The remaining four states (California, District of Columbia, Georgia, and North Carolina) had either mixed results or no difference in academic gains. The new methods developed for this study also explicitly accounts for the variation between states by comparing states in relation to rankings of charter school laws. This analysis provides insight into implications of the varied policy environments in which charter schools operate.

The \citeA{credo} also found variation in charter school effectiveness across school characteristics. That is, schools that focused on elementary or middle grades separately tended to perform as well or better than their public school counterparts. However, charter schools that focused on high grades or multi-level grades performed anywhere from .02 to .08 standard deviations below public schools. Moreover, school level comparisons find that only 17\% of charter schools performed better than public schools while 46\% perform no differently and 37\% perform significantly worse.

The results from the 2013 study \cite{credo2013} showed a small increase in the differences between charter and traditional public school students. The Center for Research on Education Outcomes prefers to present their results in a metric of school days. In 2009, charter school students had a loss of 7 school days which increased to a gain of 8 school days in 2013. For math, charter school students had a loss of 22 days in 2009 and were on par with traditional public school students in 2013. Reporting in terms of days is problematic since it is difficult to compare to other studies. More typically, standardized effect sizes are used to express the difference between two groups. \citeA{Loveless2013} re-expressed these differences as effect sizes estimating that they are roughly equivalent to an effect size between 0.01 and 0.03. These results, as shown in chapter four, are consistent with the results of this study. 

\subsection{The Role of State Charter Laws}

Charter schools must operate in accordance with the laws governing their home state. In 2009, the the National Alliance for Public Charter Schools (NAPCS) published a report, \textit{A New Model Law for Supporting the Growth of High-Quality Public Charter Schools}, in part to reflect what had been learned since the first charter school law was passed in Minnesota. This report serves as a model charter school law for each state to adapt for their use in creating or modifying their state law. The NAPCS argues that the quality of charter school laws influences the quality of charter school education in each respective state. NAPCS (2009) acknowledges that there are other key components as well. Specifically, NAPCS lists five ``primary ingredients" \cite[p. 1]{NAPCS2009} for a successful charter school: (1) Supportive laws and regulations (both what is on the books and how it is implemented); (2) Quality authorizers; (3) Effective charter support organizations, such as state charter associations and resource centers; (4) Outstanding school leaders and teachers; and (5) Engaged parents and community members.

After this report was published, NAPCS began publishing annual scores and rankings of state charter school laws in order to quantify each state's adherence to their model law. In determining the rankings, they evaluate existing charter school laws against 20 ``essential components of good charter school laws" \cite[pp. 6--7]{NAPCS2012}. It should be noted that an ``authorizer" is a legally defined entity that can grant charters to schools. The 20 points from this document are:

\begin{singlespace}\normalsize
\begin{enumerate}[noitemsep]
{\it
\item No caps.
\item A variety of public charter schools allowed.
\item Multiple authorizers available.
\item Authorizer and overall program accountability system required.
\item Adequate authorizer funding.
\item Transparent charter application, review, and decision-making processes.
\item Performance-based charter contracts required.
\item Comprehensive public charter school monitoring and data collection processes.
\item Clear processes for renewal, nonrenewal, and revocation decisions.
\item Educational service providers allowed.
\item Fiscally and legally autonomous schools.
\item Clear student recruitment, enrollment and lottery procedures.
\item Automatic exemptions from many state and district laws and regulations.
\item Automatic collective bargaining exemption.
\item Multi-school charter contracts and/or multi-charter contract boards allowed.
\item Extra-curricular and interscholastic activities eligibility and access.
\item Clear identification of special education responsibilities.
\item Equitable operational funding and equal access to all state and federal categorical funding.
\item Equitable access to capital funding and facilities.
\item Access to relevant employee retirement systems.
}
\end{enumerate}
\end{singlespace}

\noindent \hyperref[appendixL]{Appendix L} provides the rubric used by NAPCS to determine the score for each state law. Specifically, each component is rated on a scale ranging from 0 to 4. Components are assigned a weight ranging from 1 to 4 depending on their relative importance to a quality law as determined by NAPCS. The rating for each component is multiplied by the weight. The resulting scores, the sum of the weighted 20 components, range between 0 and 208.

The third purpose of this study is to explore the relationship between charter school performance and quality of state charter laws, as determined by the NAPCS scores. The multilevel PSA methods developed for this study provide effect sizes (standardized mean differences) for each state. These differences are preferable to using only charter school scores because these differences adjust for the overall variation in educational quality across states. That is, the differences are relative to traditional public school students within each state. The relationship between the quality of charter laws and differences between charter and traditional public schools is be explored using scatter plots and correlations using the NAPCS ratings and effect sizes.

\subsection{Propensity Score Analysis}

Randomized experiments are the \textit{gold standard} for estimating causal effects of a treatment. However, as is frequently the case in educational contexts, randomization for the current project is neither ethical nor feasible. Therefore, propensity score methods \cite{RosenbaumRubin1983} using matching \cite{StuartRubin2008,Stuart2010} and stratification methods \cite{RaudenbushHongRowan2003} are used to make quasi-experimental estimates of causal effects \cite<see also>{SchneiderEtAl2007,StuartRubin2008}. Propensity scores are defined more thoroughly in Chapter 3, but in brief, propensity score analysis (PSA) is a quasi-experimental method used to adjust selection bias in two phases. In the first phase, treatment and control units with similar covariate profiles (using observed covariates) are matched or clustered. The goal is to eliminate or minimize the differences in the observed covariates. When the differences between treatment and control units are minimized between matched pairs or clusters, differences in the dependent variable are calculated in phase two. Those differences are then aggregated to provide an overall estimated effect size.
% TODO: revisit the above paragraph one more time

Recent research comparing the use of propensity score methods with randomized experiments have shown that causal estimates from observational studies using propensity score methods are generally consistent with those from randomized experiments \cite{CookShadishWong2008,ShadishClarkSteiner2008}. The use of propensity score methods in published research in psychology and education has been growing over the last decade \cite{ThoemmesKim2011}. Using the Web of Science database (Thomson Reuters, 2014), the number of articles published containing the keywords ``propensity score," ``propensity score analysis," and ``propensity score matching" were extracted. Figure \ref{fig:PSApublications} depicts the number of articles published with these keywords from 1993 through 2013. Clearly, the number of publications using propensity score methods is increasing steadily over the last decade.

\begin{figure}[t]
\begin{center}
\includegraphics[width=.9\textwidth]{../Figures/PSACitations}
\caption[Number of PSA Publications by Year]{Number of PSA Publications by Year (source: Web of Science)}
\label{fig:PSApublications}
\end{center}
\end{figure}

The selection of covariates is particularly challenging in propensity score analysis. As such, multiple methods for the estimation of propensity scores are used \cite{Rosenbaum2012}. The goal in the estimation of propensity scores is to reduce selection bias, therefore simple significance testing is not appropriate \cite{Rosenbaum2002,Rosenbaum2010} since potentially non-significant covariates may be proxies for important non-observed covariates. Although this study has been designed to measure the same covariates as would be used in a randomized block design, multiple methods utilizing varying number of covariates are used. Moreover, I wish to provide overall effect sizes but also measure the effects of clustering by state. 

Although propensity score analysis has been shown to provide estimates consistent with randomized experiments \cite{ShadishClarkSteiner2008,DehejiaWahba1999,HeckmanEtAl1997}, its use has not been immune to criticism \cite<c.f.>{Shadish2013}. \citeA{Pearl2009} has raised concerns about a potential increase in bias due to the inclusion of certain covariates in the estimation of propensity scores in response to \citeauthor{Rosenbaum2002}'s \citeyear{Rosenbaum2002,Rosenbaum2010} suggestion that, in general, the inclusion of all observable covariates is preferable to excluding them. However, Pearl's concerns can be mitigated in at least three ways. First, careful checking of balance across all observable covariates is done even if the covariate is not used in the modeling of the propensity scores. Second, sensitivity analysis \cite{Rosenbaum2002,Rosenbaum2010} can be conducted after matching to consider the question of whether the estimate would differ in the presence of additional unobserved covariates. That is, sensitivity analysis tests the robustness of the propensity score estimation for hidden bias. However, sensitivity analysis is only well defined for one-to-one matching and therefore is not used in this study. Third, multiple methods can be utilized for estimating propensity scores \cite{Rosenbaum2012}. Specifically, in addition to matching based upon propensity scores estimated from logistic regression, stratification methods using both quintiles on the logistic regression estimated propensity scores and classification trees provide parametric and non-parametric estimates, respectively. 


The use of propensity score methods is still preferable to traditional regression models in spite of \citeauthor{Pearl2009}'s \citeyear{Pearl2009} criticisms. Propensity score analysis separates the covariates related to selection bias and the comparison of the outcome of interest. This clean separation also allows for a clear interpretation of results similar to randomized experiments. As will be discussed more fully in chapter three, special emphasis must be placed on achieving balance. In the context of propensity score analysis, balance refers to reducing bias or differences between observed covariates for the units that are compared. For example if, after matching each matched pair has the same ethnicity, one might conclude that perfect balance has been achieved for that covariate. In chapter three I outline a number of approaches for checking balance for both matching and stratification methods. Although there is the possibility of a lack of balance in an unobserved covariate (i.e. ``hidden bias"), this would similarly affect regression methods and is admittedly an important limitation of any non-randomized method for estimating causal effects. However, NAEP is designed to include most, if not all, the important covariates one would expect to be related to charter school attendance (see \hyperref[appendixB]{Appendix B} for descriptive statistics for all the covariates used in this study).

\subsection{Research Questions}

Clearly charter schools have become a significant part of education in the United States. Given their position as an alternative to traditional public schools, it is important to have good research about their effectiveness. Given that randomized trials are not possible, propensity score methods may provide the best alternative for making causal estimates of the differences between students who attend charter schools and their traditional public school counterpart. Moreover, the new methods developed for this study provide further insight into the extent of the variation among charter schools as well as the variation between states. As such, this study addresses the following research questions:

\begin{enumerate}
\item Given appropriate adjustments based on available student data, is there a discernible difference between charter and traditional public schools with regard to math and reading scores on the NAEP scores at grades 4 and 8? 
\item If so, what is the nature and magnitude of this difference for the two outcomes, reading and mathematics scores?  
\item What is the relationship, if any, of different charter school laws on charter school student performance in math and reading at grades 4 and 8?  
\end{enumerate}

%==================== CHAPTER 3 ====================================================================
\cleardoublepage
\section{Chapter 3: Method}

This chapter outlines the methods that were utilized to describe and analyze the data in order to address the research questions central to this study. Given the strong political interests in the question of charter school effectiveness and the implications for educational policy both at the state and national level, obtaining good empirical evidence, preferably with strong causal inferences, is most desirable. The \textit{gold standard} of inferential research about treatment differences is the randomized experiment. A research design that addresses the research questions proposed here would require that students be randomly assigned, possibly with blocking on key covariates, to either a charter or public school. The theoretical justification for such a scheme is that any systematic differences between the two groups would be balanced through the randomization processes. However, in practice, especially in education, such randomization is neither feasible nor ethical. The result of the lack of randomization is a phenomenon called selection bias. That is, any comparisons of the two groups will be biased given the fact that the units of study, in this case students, self-selected to be in their respective group. Propensity score analysis \cite{RosenbaumRubin1983} is a statistical approach whereby the observed differences between the two groups are balanced by the careful analysis of covariate information. This procedure lends itself well to secondary analysis of observational data.

I have written and published two R packages primarily for conducting the analysis in this dissertation. The \texttt{naep} package provides functions to read and work with the National Assessment of Educational Progress (NAEP) data sets. Secondly, the \texttt{multilevelPSA} package provides functions to conduct multilevel propensity score analysis as described below. Both of these packages are available from the Comprehensive R Archive Network (CRAN). 

\textit{Formatting note}. Since the development of the R packages are a major component of this dissertation, I make reference to some of the functions available. By convention, all references to R packages and functions appear in a \texttt{fixed width font}.

\subsection{Overview of NAEP}

The source of the data utilized in this study is the National Center for Educational Statistics (NCES), which is within the U.S. Department of Education's Institute of Education Sciences (IES). The National Assessment of Educational Progress (NAEP) was started in 1971 and has provided national measures of student achievement in many subjects including mathematics, reading, science, writing, history, civics, and the arts. In 2003 NAEP began assessing charter schools as well as private and public schools. This study utilizes the 2009 administration of the NAEP assessments in mathematics and reading in grades four and eight. The 2009 assessment included over 6,000 public schools and over 200 charter schools comprising over 145,000 and 3,000 students, respectively. Given this large, nationally representative sample, analysis of NAEP assessments utilizing propensity score analysis may provide valuable insights into the academic differences between charter and public schools, should they exist.

Another key advantage of NAEP is the fact that it is not designed to assess individual students or schools, but rather to inform subject-matter achievement, instructional experiences, and school environments \cite{BraunJenkinsGrigg2006}. To achieve this goal, NAEP utilizes a complex item-sampling design such that individual students are presented a subset of the total items, thereby reducing the burden on participants. Though not appropriate for assessing individual student achievement, in aggregate, the NAEP measures provide a robust, accurate, and reliable estimate of student achievement \cite{DeptOfEd2009}.

In addition to subject area measures, NAEP includes student, teacher, and school questionnaires that provide contextual information about the learning environment. Given that PSA relies on adjusting for selection bias by adjusting for known covariates, it is the answers to these questionnaires that serve as the basis for determining a student's propensity score, or likelihood of being in the treatment (charter school in the context of this study). In addition to typical demographic items such as gender and race, students are asked about computers, books, magazines, and encyclopedias in the home; parents' education level; and the level of interaction with academics within the home (see \hyperref[appendixC]{Appendix C} for complete list of items).

The responsibility for developing the assessment objectives and test specifications lies with the National Assessment Governing Board, which was created by Congress in 1988. The 26-member board is made up of governors, state legislators, local and state school officials, educators and researchers, business representatives, and members of the general public. Given the varied standards across states, it is this governing board that is to determine the appropriate achievement goals for each age and grade. The following two sections provide the framework for mathematics and reading assessments, respectively. These frameworks ensure that NAEP is a valid and reliable assessment of the academic achievement of students in the United States.

\subsubsection{Mathematics}

Since 1990, the Council of Chief State School Officers (CCSSO) has been contracted to design a framework for the mathematics assessment \cite{naepmath}. The framework was most recently updated in 2000 to take into account state standards, the National Council of Teachers of Mathematics (NCTM) standards, the Trends in International Mathematics and Science Study (TIMSS), the Achieve Project, and a 2001 report issued by the National Research Council of the National Academy of Sciences. The result of their work was six recommendations for the mathematics assessment regarding content areas, mathematical complexity of items, distribution of items, item formats, manipulatives, and calculators. For the purposes of this study, a composite score is utilized that is comprised of five content areas, number properties and operations; measurement; geometry; data analysis and probability; and algebra. Table \ref{naepMathContent} provides details regarding the distribution of items comprising the composite score for the grade four and eight assessments.

\input{../Tables/Chapter3.table.DistributionOfMathItems.tex}

\subsubsection{Reading}

The NAEP reading assessment \cite{naepreading} is designed to account for three reading contexts: reading for literacy experience, reading for information, and reading to perform a task. Within these contexts, four aspects of reading are considered: forming a general understanding, developing interpretation, making reader/text connections, and examining content and structure. The reading assessment is administered by supplying students with booklets that contain reading materials and comprehension questions. The questions consist of both multiple-choice and constructed-response question formats with at least half of the questions being of the constructed-response type. 

The \citeauthor{naepreading} NAEP Reading Framework (2006) provides guidelines and a theoretical basis for reading assessment. This framework is designed with the input of individuals and organizations involved in reading education including researchers, policymakers, teachers, and business representatives. However, a particular emphasis is placed on the work of the National Institute for Child Health and Human Development (NICHID). According to the NICHID  the cognitive research suggests that ``reading is purposeful and active. According to this view, a reader reads a text to understand what is read, to construct memory representations of what is understood, and to put this understanding to use" \cite<p. 4, NICHD, 2000, as cited in>{naepreading}. Moreover, reading is considered to be a complex process rather than a simple set of skills. As such, the NAEP reading assessment is designed such that comprehension is defined as: ``[I]ntentional thinking during which meaning is constructed through interactions between text and reader" (Harris \& Hodges, 1995). Thus, readers derive meaning from text when they engage in intentional, problem solving thinking processes \cite<NICHD, 2000, as cited in>{naepreading}. Given this framework, NAEP provides an excellent tool for evaluating overall reading achievement but not to specific individuals.

\subsection{Reliability}


For constructed response items, interrater reliability statistics are obtained by having multiple rates double score a sample of items as well as reused items evaluated across years. Cohen's Kappa \cite{Cohen1968} is used for dichotomized items and interclass correlation is used for polytomously scored items. Cohen's Kappas and the interclass correlations range from 0.80 to 0.99, with the vast majority of reliability statistics greater than 0.90 indicating very good interrater reliability.

\subsection{Sample}

NAEP is designed to be a representative sample both at the national and state levels. NCES utilizes a multistage random sample whereby public schools are randomly sampled and then students within those selected schools. To ensure an unbiased sample, NCES and the Governing Board have established participation rates at both the school and student level of 85\%. The participation rates for schools were 98\% and 97\% for grades 4 and 8, respectively. Student participation rates were 95\% and 92\% for grades 4 and 8, respectively.

Since not all students have an equal probability of being randomly sampled (e.g. students in smaller schools are slightly more likely to be sampled than students in large schools), NCES provides sampling weights to adjust for the sampling design. However, the use of sampling weights is not well established for propensity score analysis \cite<c.f.>{Gelman2007}. As such, the sampling estimates are not utilized in any phase of the propensity score analysis. It should be noted that NCES has been oversampling charter schools since the 2003 implementation of NAEP. This is advantageous for this study as it provides a larger sample from which inferences can be made. As described in detail below, not all traditional public schools are utilized in this sample. Specifically, only traditional public schools located within five miles of a charter school are used in the comparison group.

As a result of this sampling method, the 2009 sample included approximately of 4,000 and 160,000 charter and traditional public school students, respectively. \hyperref[appendixA]{Appendix A} lists the number of students by state and \hyperref[appendixB]{Appendix B} provides the descriptive statistics for all the observed characteristics for charter and traditional public school students separately.

The propensity score methods use the available covariates to adjust for selection bias. However, the geographic distribution of charter schools is not equal. That is, charter schools are more prevalent in certain geographic regions of the country, often within urban areas. Since there are several orders of magnitude difference in the number of charter school to traditional public school students, selecting a subset of all the traditional public school students available in NAEP is necessary. By selecting traditional public school students who live in close proximity to a charter school, the likelihood of the former actually choosing a charter school increases. According to the National Household Travel Study, students travel an average of five miles to school. The Common Core of Data \cite{ccd} provides the location of every public school in the United States. For each traditional public school, the distance to the closest charter school was calculated using line-of-sight distance. Within states that have charter schools, approximately 20\% of traditional public schools were more than five miles from a charter school. Those schools, and subsequently students attending those schools in any of the NAEP datasets, were eliminated from the study. Table \ref{dependentDescriptivesAllAndClose} provides descriptive statistics for charter school students, all public school students, and public school students within five miles on each of the outcome measures. It shows that there is not a substantial difference in the mean scores for traditional public school students and in fact, reduces the unadjusted differences between charter and traditional public school students.

\input{../Tables2009/descriptivesClose.tex}

\hyperref[appendixB]{Appendix B} provides descriptive statistics for all the covariates for the four datasets. Additionally, unadjusted differences in NAEP scores for each state containing a charter school are provided. Table \ref{dependentDescriptives} below provides the overall, unadjusted, differences in NAEP scorers for the four datasets.

\input{../Tables2009/descriptives.tex}

\subsection{Missing Data Imputation}

Logistic regression, which is one of the two ways propensity scores are estimated, requires a complete dataset for estimation. \hyperref[appendixD]{Appendix D} provides figures created using the \texttt{missing.plot} function in the \texttt{multilevelPSA} package representing the extent of missingness for each covariate within each state. The first thing these figures reveal is that there is complete missingness in the majority of covariates for Alaska in grade four. As a result, Alaska was removed from all datasets and was not included in the study. 

Secondly, the figures show that there are fewer than 5\% of values missing for the vast majority of covariates. In grade four math and reading, the three exceptions are newspapers in home, magazines in home, and encyclopedia in home. Grade eight math and reading also show a higher rate of missingness in these three covariates, but also in parents' education level. To examine whether data is missing at random, a logistic regression model was estimated predicting treatment from a shadow matrix (i.e. a matrix with the same dimension of the original dataset with 0s and 1s where 1s indicate the value is missing). 

The results of these models indicate that there is no relationship between charter school attendance and whether a student completed items regarding newspapers, magazines, and encyclopedias in the home. However, for grade eight math and reading, missingness of mother's and father's education level were statistically significant (\textit{p} \textless .05) predictors of treatment. Charter school students' mother's education level was less likely to be missing whereas father's education level was less likely to be missing for traditional public schools. Although these two covariates are often important for understanding students' educational achievement, the figures in \hyperref[appendixI]{Appendix I} depicting the relative importance of each covariate for predicting charter school attendance using conditional inference trees, which are estimated with missingness included, suggest that these covariates have relatively low, or no, predictive value for charter school attendance. Therefore, missing values for these covariates are imputed and used to estimate propensity scores for the logistic regression and matching methods. It should also be noted that this variable was not collected from fourth grade students.

Multiple imputation \cite{rubin1987,Rubin1996mi} has become a popular approach for imputing missing values in datasets. For this study, missing data was imputed using multivariate imputation by chained equations vis-\`{a}-vis the MICE package \cite{mice,vanbuuren} in R. This package implements the fully conditional specification (FCS) method of imputation whereby separate multivariate imputation models are estimated for each variable containing missing values so that each model has its own set of conditional densities. Since the algorithm iterates through the data in small steps, providing the imputed values as it proceeds, the result is a robust estimate of imputed values. This study used both the original incomplete data for estimating propensity scores with classification trees and the complete imputed data for estimation of propensity scores using logistic regression.

%\clearpage

\subsection{Analysis}

This study utilizes propensity score analysis for estimating causal effects of students attending charter schools. The propensity score is ``the conditional probability of assignment to a particular treatment given a vector of observed covariates" \cite{RosenbaumRubin1983}. The probability of being in the treatment is defined as:

\begin{equation}
\pi ({ X }_{ i }) \; \equiv \; Pr({ T }_{ i } = 1 | { X }_{ i })
\end{equation}

\noindent Where $X$ is a matrix of observed covariates and $\pi ({ X }_{ i })$ is the propensity score. The balancing property under exogeneity states that,

\begin{equation}
{ T }_{ i } \; \upModels { X }_{ i } \;| \; \pi ({ X }_{ i })
\end{equation}

\noindent Where $T_i$ is the treatment indicator for subject $i$. In the case of randomized experiments, the strong ignobility assumption states,

\begin{equation}
({ Y }_{ i }(1),{ Y }_{ i }(0)) \; \upModels \; { T }_{ i }|{ X }_{ i }
\end{equation}

\noindent for all ${X}_{i}$. That is, treatment is independent of all covariates, observed or otherwise. However, the strong ignobility assumption can be restated with the propensity score as,

\begin{equation}
({ Y }_{ i }(1),{ Y }_{ i }(0)) \; \upModels \; { T }_{ i } \; | \; \pi({ X }_{ i })
\end{equation}

\noindent so that treatment placement is ignorable given the propensity score presuming sufficient balance\footnote{Balance in the context of PSA refers to differences in observed covariates between treatment and control units is minimized.} is achieved.

The average treatment effect (ATE) is defined as $E(r_1) - E(r_0)$ where $E(.)$ is the expected value in the population. Given a set of covariates, $X$, and outcomes $Y$, where 0 denotes traditional public school student and 1 denotes charter school student, ATE is defined as:

\begin{equation}
ATE=E(Y_{1}-Y_{0}|X)=E(Y_{1}|X)-E(Y_{0}|X)
\end{equation}
 
\noindent Or the difference between charter and traditional public school given the set observed covariates.

For matched analysis, the ATE is the mean of the differences between each matched treatment and control. For stratification methods, the mean difference for each stratum is calculated first, then a weighted mean (weighted by the \textit{n} in each stratum) of the differences across stratum are calculated. The ATE for PSA is a direct analog of the ATE for randomized experiments. However, for randomized experiments the difference between the mean of the treatments and the mean of controls is calculated. For observational studies, and PSA in particular, differences between matched pairs or within stratum are calculated first, then the mean of the differences, so to adjust for the observed bias.

\citeA{Rosenbaum2012} suggests that hypotheses should be tested more than once in observational study. This study estimates treatment effects using nine separate propensity score models within three larger classes. The first two classes of PSA, stratification and matching, are well established in the PSA literature. The third class of PSA, multilevel PSA, is implemented in the \texttt{multilevelPSA} R package, which was developed for this dissertation. With these three classes of PSA, results reflect: (1) no adjustment for states (stratification), (2) implicit adjustment for states (matching), and (3) explicit adjustment for states (multilevel PSA). For each of the four subject and grade combinations, the following methods are used, resulting in a total of 36 propensity score analyses being conducted.

\begin{enumerate}
\item Propensity score analysis using stratification. This method ignores state assignment as a clustering variable. Under this broader method three statistical methods for stratification used are:
	\begin{enumerate}
	\item Full logistic regression. This method estimates propensity scores using logistic regression with all available covariates, but exclude interaction or product terms.
	\item Logistic regression with step AIC. The step AIC in the MASS package \cite{mass} select the best logistic model based upon the Akaike Information Criterion \cite{Akaike1974}. In this case the ``best" first order interaction terms are added to the main effect terms in (a).
	\item Conditional inference trees, based on all covariates; missing data are also accommodated with the tree-based methods.
	\end{enumerate}
\item Propensity score matching. This method implicitly accounts for clustering. That is, the method first finds matches between charter and traditional school students that match exactly on state, ethnicity, and gender, then finds a best match based upon the propensity scores estimated using logistic regression. As suggested by \citeA{Stuart2010}, multiple matched sets are formed using:
	\begin{enumerate}
	\item One-to-one (i.e. one charter school student is matched to no more than one traditional public school student).
	\item One-to-five (i.e. one charter school student is matched to as many as five traditional public school students).
	\item One-to-ten (i.e. one charter school student is matched to as many as ten traditional public school students).
	\end{enumerate}
	A dependent sample analysis is performed on the resulting matched pairs \citeA{Austin2011}.
\item Multilevel propensity score analysis \cite<see e.g.>{multilevelPSA}. This method utilizes the same stratification methods as described in method one above, namely:
	\begin{enumerate}
	\item Full logistic regression.
	\item Logistic regression with step AIC.
	\item Conditional inference trees.
	\end{enumerate}
\end{enumerate}


\subsubsection{Graphical Representation}

Given the large amount of data to be summarized, the use of graphics are an integral component of representing the results. \citeA{PruzekHelmreich2009} introduced a class of graphics for visualizing dependent sample tests \cite<see also>{granova,granovaGG}. This framework was then extended for propensity score methods using stratification \cite{HelmreichPruzek2009}. In particular, the representation of confidence intervals relative to the unit line (i.e. the line $y=x$) provided a new way of determining whether there is a statistical significant difference between two groups. The \texttt{multilevelPSA}\footnote{The \texttt{multilevelPSA} package was developed by the author and is available from \url{http://github.com/jbryer/multilevelPSA}.} package provides a number of graphing functions that extend these frameworks for multilevel PSA. Figure \ref{fig:g8math:circ} represents a multilevel PSA assessment plot with annotations. This graphic represents the results of comparing private and public schools in North America using the Programme of International Student Assessment \cite<PISA;>{pisa}. The PISA data to create this graphic are included in the \texttt{multilevelPSA} package and a more detailed description of how to create this graphic are discussed at the end of this chapter. Additionally, the use of PISA makes more visible certain features of the graphics used. As discussed in chapters four and five, the differences between charter and traditional public schools is minimal and therefore some features of the figures are less apparent. The following section focuses on the features of this graphic.

In Figure \ref{fig:g8math:circ}, the x-axis corresponds to math scores for private schools and the y-axis corresponds to public school maths cores. Each colored circle (a) is a country with its size corresponding to the number of students sampled within each country. Each country is projected to the lower left, parallel to the unit line, such that a tick mark is placed on the line with slope -1 (b). These tick marks represent the distribution of differences between private and public schools across countries. Differences are aggregated (and weighted by size) across countries. For math, the overall adjusted mean for private schools is 487, and the overall adjusted mean for public schools is 459 and represented by the horizontal (c) and vertical (d) blue lines, respectively. The dashed blue line parallel to the unit line (e) corresponds to the overall adjusted mean difference and likewise, the dashed green lines (f) correspond to the confidence interval. Lastly, rug plots along the right and top edges of the graphic (g) correspond to the distribution of each country's overall mean private and public school math scores, respectively.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=.9\textwidth]{../Figures/AnnotatedCircPlot.pdf}
\caption[Annotated multilevel PSA assessment plot]{Annotated multilevel PSA assessment plot. This plot compares private schools (\textit{x}-axis) against public schools (\textit{y}-axis) for North America from the Programme of International Student Assessment.}
\label{fig:g8math:circ}
\end{center}
\end{figure}


Figure \ref{fig:g8math:circ} represents a large amount of data and provides insight into the data and results. The figure provides overall results that would be present in a traditional table, for instance the fact that the green dashed lines do not span the unit line (i.e. y = x) indicates that there is a statistically significant difference between the two groups. However additional information is difficult to convey in tabular format. For example, the rug plots indicate that the spread in the performance of both private and public schools across countries is large. Also observe that Canada, which has the largest PISA scores for both groups, also has the largest difference (in favor of private schools) as represented by the larger distance from the unit line.


\subsection{The \texttt{multilevelPSA} R Package}


All of the analyses for this study were conducted using R \cite{rdevelopment}. The use of R provides a number of important advantages. First, all of the analyses are reproducible. That is, researchers can download all the R scripts\footnote{Available on Github at \url{https://github.com/jbryer/Dissertation}.} and those with access to the restricted NAEP data\footnote{Typically data is included for the analysis to be fully reproducible. However, given the sensitive nature of the data the National Center for Education Statistics (NCES) requires a restricted license for access to the data.} can run all the analyses. However, since NAEP is not readily available to anyone, PISA data are used to demonstrate the features of the \texttt{multilevelPSA} package. Another important advantage of using R is that it is an extensible vis-\`{a}-vis R packages. Packages are collections of functions, data, and documentation designed for a specific purpose. Since the multilevel PSA methods described in this dissertation have never been conducted or implemented elsewhere, the \texttt{multilevelPSA} package was developed. As of this writing, version 1.2 is available on The Comprehensive R Archive Network (CRAN)\footnote{The CRAN package page is available at: \url{http://cran.r-project.org/web/packages/multilevelPSA/index.html}}. In this section I outline the core functionality of the \texttt{multlilevelPSA} package. \hyperref[appendixJ]{Appendix J} provides a complete list of the available functions with brief descriptions of their purpose. By convention, R commands are type faced in a fixed-width font and begin with a greater than (\texttt{>}) symbol.

To begin, the \texttt{install.packages} and \texttt{require} functions install and load the package, respectively.

\begin{verbatim}
> install.packages('multilevelPSA', repos='http://cran.r-project.org')
> require('multilevelPSA')
\end{verbatim}

The \texttt{multilevelPSA} package includes North American data from the Programme of International Student Assessment \cite<PISA;>{pisa}. This data is made freely available for research and is utilized here so that the R code is reproducible\footnote{NAEP requires a restricted use license and therefore the data is only available to qualified researchers. The R scripts for all analysis however, are available on Github at \url{http://github.com/jbryer/Dissertation}.}. This example compares the performance of private and public schools clustered by country.

\begin{verbatim}
> data(pisana)
> data(pisa.psa.cols)
\end{verbatim}

The \texttt{mlpsa.ctree} function performs phase I of the propensity score analysis using classification trees, specifically using the \texttt{ctree} function in the \texttt{party} package. The \texttt{getStrata} function returns a data frame with a number of rows equivalent to the original data frame indicating the stratum for each student.

\begin{verbatim}
> mlpsa <- mlpsa.ctree(pisana[,c('CNT','PUBPRIV',pisa.psa.cols)], 
                       formula=PUBPRIV ~ ., level2='CNT')
> mlpsa.df <- getStrata(mlpsa, pisana, level2='CNT')
\end{verbatim}

Similarly, the \texttt{mlpsa.logistic} estimates propensity scores using logistic regression. The \texttt{getPropensityScores} function returns a data frame with a number of rows equivalent to the original data frame 

\begin{verbatim}
> mlpsa.lr <- mlpsa.logistic(pisana[,c('CNT','PUBPRIV',pisa.psa.cols)],
                             formula=PUBPRIV ~ ., level2='CNT')
> mlpsa.lr.df <- getPropensityScores(mlpsa.lr, nStrata=5)
> head(mlpsa.lr.df)
  level2    ps strata
1    CAN 0.917      2
2    CAN 0.941      3
3    CAN 0.969      4
4    CAN 0.930      2
5    CAN 0.836      1
6    CAN 0.973      4
\end{verbatim}

The \texttt{covariate.balance} function calculates balance statistics for each covariate by estimating the effect of each covariate before and after adjustment. The results can be converted to a data frame to view numeric results or the \texttt{plot} function provides a balance plot. This figure depicts the effect size of each covariate before (blue triangle) and after (red circle) propensity score adjustment. As shown here, the effect size for nearly all covariates is smaller than the unadjusted effect size. The few exceptions are for covariates where the unadjusted effect size was already small. There is no established threshold for what is considered a sufficiently small effect size. In general, I recommend adjusted effect sizes less than 0.1 which reflect less than 1\% of variance explained.

\begin{verbatim}
> cv.bal <- covariate.balance(covariates=student[,pisa.psa.cols],
                              treatment=student$PUBPRIV,
                              level2=student$CNT,
                              strata=mlpsa.df$strata)
> head(as.data.frame(cv.bal))
  covariate es.adj es.adj.wtd es.unadj
1   ST04Q01 0.0565  -0.000396   0.0258
2   ST06Q01 0.0167  -0.000292   0.0796
3   ST08Q01 0.0766   0.000515   0.1014
4   ST08Q02 0.0379   0.000500   0.0913
5   ST08Q03 0.0150  -0.000850   0.0286
6   ST08Q04 0.0431  -0.000278   0.0058
> plot(cv.bal)
\end{verbatim}

\begin{figure}[h!]
\begin{center}
\includegraphics[height=.90\textheight]{../Figures/pisabalance.pdf}
\caption[Multilevel PSA balance plot for PISA]{Multilevel PSA balance plot for PISA. The effect sizes (standardized mean differences) for each covariate are provided before PSA adjustment (blue triangles) and after PSA adjustment (red circles).}
\end{center}
\end{figure}

\clearpage

The \texttt{mlpsa} function performs phase II of propensity score analysis and requires four parameters: the response variable, treatment indicator, stratum, and clustering indicator. The \texttt{minN} parameter (which defaults to five) indicates what the minimum stratum size is to be included in the analysis. For this example, 463, or less than one percent of students were removed because the stratum (or leaf node for classification trees) did not contain at least five students from both the treatment and control groups.

\begin{verbatim}
> results.psa.math <- mlpsa(response=mlpsa.df$MathScore, 
                            treatment=mlpsa.df$PUBPRIV, 
                            strata=mlpsa.df$strata, 
                            level2=mlpsa.df$CNT)
Removed 463 (0.696%) rows due to stratum size being less than 5
\end{verbatim}

\noindent The \texttt{summary} function provides the overall treatment estimates as well as level one and two summaries.

\begin{verbatim}
> summary(results.psa.math)
Multilevel PSA Model of 85 strata for 3 levels.
Approx t: -10.8
Confidence Interval: -31.3, -24.75
   level2  strata Treat Treat.n Control Control.n ci.min ci.max
1     CAN Overall   579    1625     513     21093  -72.1 -59.57
2    <NA>       1   580      28     492      1128     NA     NA
3    <NA>       2   600       9     476      1326     NA     NA
... # Output truncated to save space
\end{verbatim}

\noindent The \texttt{plot} function creates the multilevel assessment plot. Here it is depicted with side panels showing the distribution of math scores for all strata for public school students to the left and private school students below. These panels can be plotted separately using the \texttt{mlpsa.circ.plot} and \texttt{mlpsa.distribution.plot} functions.

\begin{verbatim}
> plot(results.psa.math)
\end{verbatim}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\textwidth]{../Figures/pisamlpsa.pdf}
\caption[Multilevel PSA assessment plot for PISA]{Multilevel PSA assessment plot for PISA. The main panel provides the adjusted mean for private (\textit{x}-axis) and public (\textit{y}-axis) for each country. The left and lower panels provide the mean for each stratum for the public and private students, respectively. The overall adjusted mean difference is represented by the dashed blue line and the 95\% confidence interval by the dashed green lines. There is a statistical significant difference between private and public school student performance as evidenced by the confidence interval not spanning zero (i.e. not crossing the unit line \textit{y}=\textit{x}.}
\end{center}
\end{figure}

\clearpage
Lastly, the \texttt{mlpsa.difference.plot} function plots the overall differences. The \texttt{sd} parameter is optional, but if specified, the \textit{x}-axis can be interpreted as standardized effect sizes.

\begin{verbatim}
> mlpsa.difference.plot(results.psa.math, 
                        sd=mean(mlpsa.df$MathScore, na.rm=TRUE))
\end{verbatim}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\textwidth]{../Figures/pisadiffplot.pdf}
\caption[Multilevel PSA difference plot for PISA]{Multilevel PSA difference plot for PISA. Each blue dot corresponds to the effect size (standardized mean difference) for each country. The vertical blue line corresponds to the overall effect size for all countries. The green lines correspond to the 95\% confidence intervals. The size of each dot is proportional to the sample size within each country.}
\end{center}
\end{figure}

\subsection{State Charter Laws and Charter School Performance}

Due to the explicit adjustment within clusters in multilevel PSA, a natural means of comparing clusters is available. That is, since effect sizes are estimated for each cluster, clusters can be compared on those estimated effect sizes. In the process of estimating a national effect size, an effect size for each individual state is also estimated. As such, states can be compared with regard to the effectiveness of charter school performance within that state to some other state level indicators.

This study compares the effect sizes from the multilevel PSA with the quality of state charter school law scores from NAPCS. Specifically, the correlation between the NAPCS scores for the quality of charter schools laws and the state level effect sizes of charter schools are calculated. Although this analysis is simply correlational, no known analysis exploring the relationship between charter law and student performance has been conducted despite the fact that \citeA{NAPCS2009} argues that the quality of charters laws are necessary for higher achieving charter schools. Additionally, in keeping with the emphasis of visualizations, scatter plots are provided.

The \citeA{NAPCS2010a} rates the quality of charter schools using a 4-point rubric across 20 components (see \hyperref[appendixL]{Appendix L}). The resulting ratings have a maximum value of 208. These scores are compared to the effect sizes from the multilevel PSA (using standardized mean differences). The correlation for each grade and subject are evaluated to be small, moderate, or strong using \citeauthor{Cohen1988}'s \citeyear{Cohen1988} thresholds of 0.10, 0.30, and 0.50, respectively.



%==================== CHAPTER 4 ====================================================================
\cleardoublepage
\section{Chapter 4: Results}

This chapter outlines in detail the results of all the propensity score models described in chapter three. Since NAEP is organized such that each grade and subject combination is a separate dataset, this chapter focuses on the analysis of grade four math. The results for grade four reading, grade eight math, and grade eight reading are included in the appendices. The chapter begins with details of the nine propensity score methods used and concludes with a summary, including tables and figures, of the overall results across all grades and subjects.

\subsection{Propensity Score Analysis with Stratification}

The first class of propensity score methods used was stratification. The general approach of stratification methods is to subdivide the available sample into smaller groups that have similar covariate profiles. Then a comparison using mean differences between the treatment and control are made, and an overall result is pooled from those individual comparisons. There are several ways to stratify the sample: for this study deciles based upon the propensity scores (i.e. fitted values of a logistic regression model) and leaves of a fitted classification tree were used. Moreover, given the importance of covariate selection and omission from propensity score models, two types of logistic regression models were used, namely a full model using all available covariates and an Akaike Information Criterion \cite<AIC;>{Akaike1974} optimized model. The latter is determined by a stepwise model selection algorithm where covariates are added and dropped and the model that optimizes the AIC is retained. Like all analysis in phase one, this was done without outcome variables.

\setlength{\belowcaptionskip}{-10pt}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-loess.pdf}
\caption[Loess regression assessment plot: Grade 4 math]{Loess regression assessment plot: Grade 4 math. The upper panel provides the distributions of propensity scores. The right panel provides the unadjusted distributions of dependent variable grade 4 NAEP math score. The main panel plots each students' propensity score against their outcome variable. Two Loess regression lines are provided for charter and traditional public schools. The grey bands correspond to the 95\% confidence interval. Given that the confidence intervals overlap across the entire range of propensity scores provides evidence that there is little or no difference in the performance between charter and traditional public school student.}
\label{fig:g4math:loess}
\end{center}
\end{figure}
\setlength{\belowcaptionskip}{0pt}

However, before stratifying the logistic regression models, I examined the relationship between propensity scores and the outcome variable for the two groups and fitted a Loess regression line to the scatter plot to provide an overall indication of the differences, if any. Figure \ref{fig:g4math:loess} is a Loess Regression Assessment Plot created using the loess.plot function in the \texttt{multilevelPSA} package.\footnote{This function is adapted from the \texttt{loess.psa} function from the \texttt{PSAgraphics} package \cite{HelmreichPruzek2009}. This version implements the figure utilizing the grammar of graphics framework \cite{Wilkinson2005} implemented in the  \texttt{ggplot2} \cite{Wickham2009} R package.} The main panel is a scatterplot of each student's propensity score on the \textit{x}-axis and math score on the \textit{y}-axis (for clarity a random 10\% sample of data points are plotted; more than 10\% would have resulted in a graphic too dense to interpret). 

% TODO: how are the confidence intervals calculated. Added the footnote, but should explore in more detail

Two fitted Loess regression lines with approximate 95\% confidence intervals are also plotted\footnote{The confidence intervals are based upon the \textit{t}-distribution using the standard errors from the fitted values \cite{Cleveland1992}. This is implemented in the \texttt{loess} function in the \texttt{stats} package included as part of the base R \cite{rdevelopment} installation}. Loess lines are based upon the full dataset. The panel on the top provides density distributions of the propensity scores and shows that there is generally good overlap between the two groups. Having adequate overlap is critical since it indicates there are treatment and comparison units with similar propensity scores that are compared on their outcome variables. The panel on the right is a density distribution of the unadjusted outcome and shows that before propensity score adjustment, traditional public school students performed slightly better than charter school students. However, given the strong overlap in the two Loess regression lines, this figure suggests there is no discernible difference in performance between traditional public school students and charter school students in grade four math. Corresponding plots for the other datasets, as well as those for the AIC optimized models, are provided in \hyperref[appendixD]{Appendix D}.

The vertical lines in the main panel of Figure \ref{fig:g4math:loess} represent the deciles, or strata. Figure \ref{fig:g4math:circpsa} is a propensity score assessment plot \cite{HelmreichPruzek2009} where the \textit{x}-axis is the outcome score for charter schools and the \textit{y}-axis is the outcome score for traditional public schools (corresponding to the points within each vertical line in Figure \ref{fig:g4math:loess}). Each circle corresponds to each stratum and the size of the circle is proportional to the number of students within each stratum. For the Logistic regression models, since deciles were used, each circle is of the same size. Figure \ref{fig:g4math:circpsa:tree} is the corresponding propensity score assessment plot for the classification tree model and therefore each stratum is not of the same size. Points that lie on or near the unit line, $y = x$, indicate no significant difference in the outcome of the two scores. Lines are projected to a line perpendicular to the unit line and the tick placed. These tick marks correspond to the distribution of difference scores and the dashed blue line parallel to the unit line the overall mean difference. Furthermore, the green bar represent exactly the 95\% confidence interval. The fact that the confidence interval does not span the unit line and is on the tradition public school side indicates there is a small, statistically significant difference in favor of traditional public school students. Tables \ref{g4math-circpsa10}, \ref{g4math-circpsa10AIC}, and \ref{g4math-circpsa-tree} provide numeric results for each stratum. \hyperref[appendixF]{Appendix F} contains propensity score assessment plots and summary tables for grade four reading, grade eight math, and grade eight reading.

%\clearpage

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-circpsa10.pdf}
\caption{Propensity score assessment plot for logistic regression stratification: Grade 4 math}
\label{fig:g4math:circpsa}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-circpsa-tree.pdf}
\caption{Propensity score assessment plot for classification tree stratification: Grade 4 math}
\label{fig:g4math:circpsa:tree}
\end{center}
\end{figure}

\subsubsection{Covariate Balance}

The goal of propensity score methods is to adjust for selection bias with the available observed covariates, and the results discussed above are only as good as the balance achieved. In practice researchers test for the effectiveness of bias reduction by evaluating covariate balance. Perfect balance is achieved when there are no differences in covariate values for any matched pair or stratum. However, perfect balance is almost never achieved. Figure \ref{fig:g4math:balance} is a Covariate Effect Size balance plot. For each covariate on the \textit{y}-axis, the absolute standardized effect size before adjustment (in red) and after adjustment (in blue) are plotted. Effect sizes for each stratum are represented by letters. This figure shows that the propensity score adjustment greatly reduced the effects of each covariate. There is not a conventional adjusted effect size threshold for achieving sufficient balance in the PSA literature. However, more generally \citeA{Cohen1988} has suggested that an effect size between 0.2 and 0.3 would be small. As a general rule of thumb I suggest that an adjusted effect size of less than 0.1 would be very small (accounting for less than 1\% of the variance in the model) and therefore evidence of sufficient balance. The remaining covariate balance plots are provided in \hyperref[appendixE]{Appendix E}.

\input{../Tables2009/g4math-circpsa10.tex}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-lr-balance.pdf}
\caption{Covariate balance plot for logistic regression stratification: Grade 4 math}
\label{fig:g4math:balance}
\end{center}
\end{figure}


\clearpage
\input{../Tables2009/g4math-circpsa10AIC.tex}
\input{../Tables2009/g4math-circpsa-tree.tex}


\subsection{Propensity Score Matching}

The second class of propensity score method used is propensity score matching. In propensity score matching, the goal is to match students from the two groups with small differences in their propensity scores in order to adjust for selection bias. In large datasets, or when particular covariates are determined to be more important for adjusting selection bias, whether theoretically or otherwise, partial exact matching is done. In the context of this study, partial exact matching is akin to implicitly adjusting for the multilevel nature of the data. Students were first matched exactly by state, gender, and ethnicity, and then by propensity score using nearest neighbor (i.e. the difference between propensity scores of pairs is minimized). Furthermore, a caliper of 0.25 is specified which guarantees that the distance between any matched pair is no more than one-fourth of a standard deviation as suggested by \citeA{Rosenbaum2002}. Propensity scores from the full logistic regression model were used for matching.

\setlength{\belowcaptionskip}{-10pt}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-loess-matching.pdf}
\caption[Loess Plot with Matched Pairs]{Loess Plot with Matched Pairs. This is a modified version of Figure \ref{fig:g4math:loess}. A random sample of 100 matched pairs were selected from the one-to-one matching analysis. The propensity scores are plotted against the grade 4 math NAEP score for each student. The students who were matched are connected by a black line segment. The Loess regression lines, however, are estimated from the full dataset.}
\label{fig:g4math:loessmatching}
\end{center}
\end{figure}
\setlength{\belowcaptionskip}{0pt}

The \texttt{Matchby} function in the \texttt{Matching} package \cite{matching} was used to find matches. First, propensity scores were estimated using the full logistic regression model. The \texttt{Matchby} algorithm first determines which students match exactly on state, gender, and ethnicity. Within those subgroups, students with the smallest standardized difference and less than 0.25 standard deviations, are returned. Three matched sets were produced (stated as charter-to-public): one-to-one, one-to-five, and one-to-ten. Matching was done without replacement. Figure \ref{fig:g4math:loessmatching} depicts the relationship between the propensity scores and NAEP scores for matched pairs. In this figure, 100 random matched pairs were selected from the one-to-one matched analysis for grade 4 math. Note that the Loess regression lines are estimated from the full dataset. For those 100 matched pairs, their propensity scores are plotted against their grade 4 math score. The students who were matched are then connected by the black line segment. Given that all the lines are nearly perfectly vertical indicates that the difference in propensity scores is minimized. The ATE is then calculated approximately\footnote{Technically the lines are the hypotenuse of a right triangle. The ATE is calculated from the difference in scores which would be the side of the triangle parallel with the \textit{y}-axis.} as the mean of the lengths of those lines.

Once matched pairs were determined, dependent sample \textit{t}-tests were performed \cite{Austin2011} to estimate average treatment effect and corresponding confidence intervals. Figures \ref{fig:overallcirc} and \ref{fig:overalldiff} and Table \ref{tab:overall} at the end of the chapter provide the overall results. In general however, matching methods tend to estimate slightly larger treatment effects than both the stratification and multilevel models. And of additional note, the confidence intervals shrink as the ratio of treatment-to-control units increase due to the large sample \textit{n}.

By using partial exact matching, perfect balance is achieved on the covariates exactly matched on, namely state, gender, and ethnicity. Since balance was achieved using the full logistic regression model discussed above in the stratification section, by extension balance is also achieved for propensity score matching. That is, since balance was achieved for students with propensity scores within each quintile, and the fact the maximum distance between any two matched students in 0.25 standard deviations of the propensity score, then any matched pair must also be balanced.

\subsection{Multilevel Propensity Score Analysis}

The final class of propensity score method utilized is multilevel propensity score analysis. This approach to PSA was developed for this dissertation and implemented in the \texttt{multilevelPSA} R package. The multilevel PSA approach makes explicit in both phase I and II the multilevel nature of the data, in the case of this study, state. In principle, the multilevel PSA approach is a conceptual combination of the partial exact matching and stratification. However, whereas partial exact matching utilizes propensity scores estimated from a single logistic regression model, the multilevel PSA algorithm estimates separate propensity score models, using either logistic regression or classification trees (both were done for this study), for each level two cluster (i.e. state). That is, the algorithm performs \textit{m} separate propensity score analyses using stratification where \textit{m} is the number of states. This approach provides average treatment effects for each state as well as an overall, national, estimated treatment effect.

\setlength{\belowcaptionskip}{-10pt}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-mlpsa-ctree-heat.pdf}
\caption[Multilevel PSA covariate heat map for classification trees: Grade 4 math]{Multilevel PSA covariate heat map for classification trees: Grade 4 math. Each colored cell indicates that that covariate was used in the classification tree for that state. The darkness indicates the relative importance for that covariate in that state such that darker colors indicate that the covariate was used to split closer to the root node.}
\label{fig:g4math-mlpsa-ctree-heat}
\end{center}
\end{figure}
\setlength{\belowcaptionskip}{0pt}

The same three methods of stratification described above were used: full logistic regression model using all covariates, logistic regression model that optimized the Akaike Information Criterion (AIC), and classification trees. For the logistic regression models strata are defined using quintiles of the propensity scores. One difficulty in interpreting results for multilevel PSA models is the relative importance of covariates for predicting treatment. Figure \ref{fig:g4math-mlpsa-ctree-heat} is a covariate heat map that depicts each covariate on the \textit{y}-axis and state on the \textit{x}-axis. If a covariate is present in the fitted classification tree for that state, the intersecting cell is shaded. The darkness of the color represents how far down the tree that covariate first appears. That is, the darkest color indicates that the covariate was used to split the tree at the root (or the first splitting covariate). This provides an opportunity to compare the relative importance of each covariate across states. The results for grade four math show that ethnicity is the strongest predictor of treatment, having appeared in 17 of the trees, with National School Lunch eligibility as the second. For the classification tree methods, stratum with fewer than five students in either of the two groups were eliminated. Since quintiles were used for the logistic regression models, all students within those states are used. Table \ref{g4math-circpsa10} provides the results within each stratum of each state including stratum size.

\subsubsection{Covariate Balance}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-mlpsa-ctree-balance.pdf}
\caption{Multilevel PSA covariate balance plot classification trees: Grade 4 math}
\label{fig:g4math-mlpsa-ctree-balance}
\end{center}
\end{figure}

Figure \ref{fig:g4math-mlpsa-ctree-balance} is the multilevel PSA counterpart to the covariate balance plot described above. Individual stratum have been excluded for clarity since there are substantially more strata. This figure shows that, in general, relatively good balance has been achieved since the adjusted absolute effect sizes are smaller or not substantially different than the unadjusted effect sizes, and using the same criteria discussed above, all the adjusted effect sizes are smaller than 0.1. The remaining multilevel PSA covariate balance plots are provided in \hyperref[appendixG]{Appendix G}. The classification tree methods, in general, provide much better balance than the logistic regression models. This is a limitation of estimating logistic regression models with samples that have disproportional numbers of control-to-treatment students in the dependent variable. As such, interpreting the multilevel PSA logistic regression models in isolation is discouraged. However, this study follows the advice of \citeA{Rosenbaum2012} in that these are just two of the nine methods used to estimate causal effects.

\subsubsection{Visualizing Multilevel PSA}

An important advantage of multilevel PSA is that average treatment effects can be estimated for each state and then aggregated to provide a national average treatment effect. A number of graphics have been developed to help interpret these results. Figure \ref{fig:g4math-mlpsa-ctree} is a multilevel PSA assessment plot for grade four math. This is an extension of the PSA assessment plots \cite{HelmreichPruzek2009} described earlier. Each point represents the overall adjusted score for each state (the point size is proportional to the number of students sampled in each state) with traditional public schools on the \textit{x}-axis and charter schools on the \textit{y}-axis. The overall national mean scores are represented by the blue lines. The tick marks on the line perpendicular to the unit line ($y = x$) represent the distribution of differences for states. The dashed blue line\footnote{For this dataset the blue and green lines almost completely overlaps the unit line but are present.} is the overall national mean difference and the green lines are the 95\% confidence interval. This figure depicts that there is not a statistically significant difference nationally for grade four math using classification trees as evidenced by the confidence interval (the green lines) overlapping the zero (the unit line). Moreover, there is minimal difference for most states, since most of the points fall close to the unit line. However, there are some states that have a small positive effect size for charter school students  while others have a small negative effect. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-mlpsa-ctree-circ.pdf}
\caption{Multilevel PSA assessment plot classification trees: Grade 4 math}
\label{fig:g4math-mlpsa-ctree}
\end{center}
\end{figure}

Figure \ref{fig:g4math-mlpsa-ctree-diff} provides a more detailed depiction of the differences in Figure \ref{fig:g4math-mlpsa-ctree}. The tick marks in Figure \ref{fig:g4math-mlpsa-ctree} on the line perpendicular to the unit line in the lower left corner of the plot correspond to the distribution of difference scores. Figure \ref{fig:g4math-mlpsa-ctree-diff} represents only this distribution with more details. Specifically, the small grey points correspond to the difference for each stratum. The blue points are the overall difference for each state, with the point size corresponding to the number of students sampled. The 95\% confidence intervals for each state are provided in green. The overall adjusted national effect size and corresponding 95\% confidence interval are represented by the vertical blue line and vertical green lines, respectively. From this figure, charter school students performed statistically significantly better than traditional public school students in Idaho, Illinois, Georgia, and Minnesota as evidenced by the confidence intervals that do not span zero. Conversely, traditional public school students performed higher in Arizona, Texas, Massachusetts, and New Jersey. For all other states there was no statistical difference. From a national perspective, there is no difference between the performance of charter and traditional public school students as evidenced by the vertical green lines spanning zero. Figures for grade four reading, grade eight math, and grade eight reading are provided in \hyperref[appendixH]{Appendix H}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/g4math-mlpsa-ctree-diff.pdf}
\caption{Multilevel PSA difference plot classification trees: Grade 4 math}
\label{fig:g4math-mlpsa-ctree-diff}
\end{center}
\end{figure}


\subsection{Summary and Overall Results}

Up to this point in the chapter, I have outlined the nine propensity score methods used for estimating treatment effects with grade four math. The corresponding tables and figures have been referenced in the appendices. In this section, I provide two figures and one table that summarize the 36 propensity score models estimated.

A scatter plot of the overall national estimated treatment effects for all 36 PSA methods is presented in Figure \ref{fig:overallcirc}. The differences across subjects and grades are a result of different scales used for the assessment and therefore comparisons across subject and grade levels is not appropriate. The diameters of the circles in this figure are equal to the confidence interval so that circles that overlap the unit line indicate a non-significant difference. The horizontal and vertical lines (with numeric labels) represent the overall unadjusted NAEP score for traditional public school students and charter school students. This figure shows that, in general, the scores for charter school students are higher when adjusted, whereas the traditional public school scores are the same or lower. Regardless, in most cases the differences do not deviate substantially from the unit line indicating either difference in scores is small.

\setlength{\belowcaptionskip}{-10pt}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/OverallScatter.pdf}
\caption[PSA circle plot of adjusted means]{PSA circle plot of adjusted means. The vertical and horizontal lines correspond to the unadjusted means for charter and traditional public schools, respectively. The points represent the overall effect sizes (standardized mean difference) for each of the nine propensity score methods used.}
\label{fig:overallcirc}
\end{center}
\end{figure}
\setlength{\belowcaptionskip}{0pt}

Figure \ref{fig:overalldiff} provides the overall national effect sizes for each PSA method within each grade and subject. Table \ref{tab:overall} provides numeric results for this figure. Figure \ref{fig:overalldiff} reveals a number of important results. First, with regard to the effects of charter schools, there is some variety in effects across the different grades and subjects. In general, it appears charter schools either perform worse than or equal to traditional public schools in grade four. Nearly half of the grade eight models in both math and reading suggest small positive effects. However, even when there are statistically significant positive effects, the maximum effect size is relatively small (0.11). 


Figure \ref{fig:overalldiff} also reveals some trends in the behavior of the different propensity score methods. There appears to be fairly good consistency in the estimated effects within the stratification and matching methods, although in general, the matching methods provide larger effect size estimates. However, the matching methods, even with one-to-ten, use fewer than 40\% of the available traditional public school students, whereas the stratification methods use all traditional public school students. There is some variation in the estimated effect sizes for the multilevel models with the classification trees providing larger estimates. As noted above, this may be due, in part, to insufficient balance being achieved. This is likely a limitation of the logistic regression to provide stable estimates given one, the larger charter-to-public school student ratio and two, the smaller samples within each state. The following chapter provides a discussion of the implications of these results.

\setlength{\belowcaptionskip}{-10pt}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/Overall.pdf}
\caption[Overall differences in effect size]{Overall differences in effect size. The blue dots correspond to the overall effect (standardized mean difference) for each method. The green bard correspond to the 95\% confidence interval.}
\label{fig:overalldiff}
\end{center}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/LawScoresVsNAEPDifferences.pdf}
\caption[Comparison of 2010 NAPCS quality of charter law scores and NAEP charter school effect sizes]{Comparison of 2010 NAPCS quality of charter law scores and NAEP charter school effect sizes. Linear and Loess regression lines are provided in black and blue, respectively. Grey bands correspond to the 95\% confidence interval for the Loess regression. The size of the points correspond to the overall mean NAEP score (converted to a z-score) for each state.}
\label{fig:staterankings}
\end{center}
\end{figure}
\setlength{\belowcaptionskip}{0pt}


\subsection{Differences Between States}

The National Alliance for Public Charter Schools \citeyear<NAPCS;>{NAPCS2010a} publishes annual ratings and rankings of state charter school laws. The scores are based upon a rubric of 20 essential components of effective charter laws (see \hyperref[appendixL]{Appendix L}). Figure \ref{fig:staterankings} is a scatter plot for the NAPCS scores and the effect sizes from the multilevel PSA for math and reading at grades 4 and 8. The correlations are small to moderate \cite{Cohen1988} ranging from 0.095 for grade 4 math to 0.33 for grade 8 math. Additionally, the linear (formulas in lower right) and Loess regression lines in black and blue, respectively, are included. \hyperref[appendixM]{Appendix M} provides a matrix plot of the NAPCS quality of charter law scores and NAEP effect sizes. In this plot, the lower panels contain scatter plots for each pair of variables with linear and Loess regression lines in black and blue, respectively. The main diagonal contains histograms for each variable. The upper panels contain the Pearson correlation between each pair of variables. Excluding grade 4 math, there appears to be a moderate correlation between each of the charter school effect sizes and NAPCS quality of charter law scores. %It appears that grade 4 math is only correlated with grade 4 reading. 

%\begin{figure}[t]
%\begin{center}
%\includegraphics[width=\textwidth]{../Figures2009/StateRankings.pdf}
%\caption{Comparison of 2012 National Alliance for Public Charter Schools (NAPCS) state charter school law rankings and NAEP charter school rankings}
%\label{fig:staterankings}
%\end{center}
%\end{figure}


\input{../Tables2009/Overall.tex}


%==================== CHAPTER 5 ====================================================================
\cleardoublepage
\section{Chapter 5: Discussion}

This study aims to make two major contributions: first, to address the question of the effectiveness of charter schools from a state and national perspective, and second, to develop a new method of propensity score analysis for multilevel data to facilitate answering the research questions. This chapter interprets the results as well as point out some limitations of this study.

\subsection{Discussion of Research Questions}

This study set out to address three research questions regarding the differences between charter and traditional public schools. The first two questions, regarding differences in terms of student performance on NAEP, are addressed in the following section. The relationship between charter school performance and state charter school laws are discussed separately.

\subsubsection{Differences Between Charter and Traditional Public Schools}

The first research question addressed by this study was: Given appropriate adjustments based on available student data, is there a discernible difference between charter and traditional public schools with regard to math and reading scores on the NAEP evaluated at grades 4 and 8? The second question was: If so, what is the nature and magnitude of this difference for the two outcomes, in reading and mathematics? Of the 36 different propensity score models estimated, 11 resulted in a positive effect for charter schools, another 11 resulted in a positive effect for traditional public schools, and the remaining 14 resulted in no difference (see Figure \ref{fig:overalldiff} and Table \ref{tab:overall}). Across all models, effect sizes ranged from -0.16 to 0.11, all considered very small by virtually all statistical standards \cite{Cohen1988}. In aggregate, and given the available data, there is no discernible difference in the performance of charter and traditional public school students in grade 4 and 8 math and reading in NAEP.

These results must be considered in terms of a limitation of the study. Given the substantial difference in sample \textit{n}'s for charter and public schools (i.e. there are as many as three to four orders of magnitude more public school students available in the NAEP data sets), it is expected that there would be public school students who would not have a counterpart from the charter school group. However, the relatively high percentage of public schools students who do not have a charter school counterpart (as much as 35\%) suggests that there may be imbalance between the two groups as a whole. That is, although reasonable balance was achieved with regard to the individual stratum where comparisons are made, the overall sample imbalance, as evidenced by the unmatched public school students, suggests that public schools serve a more heterogeneous population. The nature of the demographic differences between charter and traditional public school students should be explored in a future study.


%Moreover, the methodological issues described below, in part, be evidence of any underlying fundamental difference in charter school and traditional public school populations. Figure \ref{fig:overalldiff} in chapter four reveals that the methods with higher effect sizes are the matching methods and classification trees. These models, especially the matching methods, often eliminate a large proportion of tradition public school students. Perhaps there may be a small positive effect of charter schools for a very specific type of student, but there also appears to be a large number of students who appear better served, or at least equivalently served, by traditional public schools. This needs to be considered when weighing the cost and extent of charter schools.

\subsubsection{State Charter Laws and Charter School Performance}

\citeA{BraunJenkinsGrigg2006} suggested that there are political and policy influences on the performance of charter schools. The National Alliance for Public Charter Schools \citeyear<NAPCS;>{NAPCS2010a,NAPCS2012} publishes ratings and rankings of state charter law quality annually, in part because they believe that poor charter school laws can hinder the performance of students in charter schools. The fact that the results of the multilevel PSA provides a natural ranking based upon student differences within each state provide an opportunity to test the relationship between the quality of state laws and student performance in NAEP. Results suggest there is a small to moderate correlation between the quality of charter laws and charter school effects in math and reading at grades 4 and 8 in NAEP (see Figure \ref{fig:staterankings}). Although this study suggests there is a relationship between the quality of charter laws and charter school performance vis-\`{a}-vis NAEP, a causal relationship cannot be concluded. Future studies should examine how the 20 components of good charter laws identified by NAPCS relate to the performance of specific charter schools.

Another limitation of this analysis is that not all states that have charter laws had sufficient sample size in NAEP to be included in the analysis (16 states, or 39\% of the 41 states with charter laws). Although the larger states and the states who have had charter schools operating for many years are included, these results should be updated with future NAEP studies as the number and percentage of charter school students grows in the states not included in this analysis.

\subsection{Discussion of Research Methods}

In order to address the research questions discussed above, a new class of propensity score method was developed for multilevel, or clustered data. The results of this study indicate that this method provides estimates that are consistent with more traditional approaches to PSA. However, this method provides important insight into the relationship of clusters not available using other methods. This section addresses the methodological implications of this study.

\subsubsection{Multilevel Propensity Score Analysis}

The development of the \texttt{multilevelPSA} R package for estimating and visualizing propensity score models of multilevel data provides important insight into the implications of what traditionally would have been one of many covariates. The results suggest that this method performs and provides effect size estimates consistent with other propensity score methods. However, a key advantage to using this new method includes an explicit adjustment of the multilevel nature of some data as well as being able to understand the implication of clustering when comparing the outcome of interest.

Propensity score methods have been effective for estimating treatment effects with relatively small samples \cite<see e.g.>{HelmreichPruzek2009}. However, as observed in chapter four, estimating multilevel PSA models requires larger samples, given the need to stratify within clusters. Although NCES began oversampling charter schools in 2003, the fact that the ratio of charter to traditional public school students is so large results in model specification problems, especially with logistic regression\footnote{Nationally, approximately 2 million students, or 4.2\%, attend charter schools \cite{ccd}.}. This has been alleviated to some extent by removing traditional public school students who attend a school farther than five miles from a charter school.

More specifically, with regard to propensity score ranges, the range tends to shrink as the ratio of treatment-to-control increases. Figure \ref{fig:psranges} depicts the range and distribution of propensity scores (using logistic regression) with varying treatment-to-control ratios. The data used to create this figure is simulated and available in \hyperref[appendixK]{Appendix K}. The \texttt{psrange} and \texttt{plot.psrange} functions are included in the \texttt{multilevelPSA} R package. Propensity scores are estimated with a single covariate where the mean for the treatment and control are 0.6 and 0.4, respectively. The standard deviation for both is 0.4. There are 100 treatment units and 1,000 control units simulated. The goal in choosing these means and standard deviations is to have some separation between treatment and control. Each row in the figure represents the percentage of control units sampled before estimating the propensity scores, starting with 100\% (i.e. all 1,000 control units) to 10\% (100 of the control units). As the figure shows, as the ratio decreases to where there are equal treatment and control units, the range of the propensity scores becomes more normal. To calculate the ranges, each sampling step is bootstrapped so the green bar and black points represent each of the 20 bootstrap samples taken. The bars then represent the mean of the minimum and mean of the maximum for each step.

The ``shrinking" of propensity score ranges as the ratio of treatment-to-control increases has implications for the interpretation of propensity scores. Typically, propensity scores are interpreted as the probability of being in the treatment. For studies where the number of treatment and control units are roughly equal, this interpretation is valid. However, in cases where the ratio of treatment-to-control is large, it best to simply interpret the propensity scores as adjustment scores and not probabilities. Since the matching and stratification procedures utilize standard scores (i.e. the propensity score divided by the standard deviation of the propensity scores), this only impacts interpretation of the propensity scores and has no impact on the estimated treatment effects.

\clearpage

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{../Figures2009/PSRanges.pdf}
\caption{Propensity score ranges for varying treatment-to-control ratios}
\label{fig:psranges}
\end{center}
\end{figure}

\subsubsection{The Display of Multilevel Results}

In the development of the \texttt{multilevelPSA}, as well as all the analyses in this study, two overarching principal decisions were made with regard to how results are displayed, specifically the lack of \textit{p}-values and an emphasis on visualizations over tabular output. Both of these issues have received substantial attention and debate over the last several decades \cite{Shrout1997,Hunter1997,Harris1997,Abelson1997,Scarr1997,Estes1997}. Although there is no clear consensus on best practice, I contend that given the nature of propensity score analysis and observational studies, simple null hypotheses reported as either statistically significant or not in tables with \texttt{p}-values does a disservice to the results. The use of graphics with confidence intervals provide context as well as magnitudes of differences.

% p-values

The practice of significance testing\footnote{Here, I use the phrases significance testing, null hypothesis testing, and \textit{p}-values to represent the same statistical practice and are generally interchangeable.} dates to the early work of \citeA{Fisher1925}. \citeA{Gigerenzer2004} describes the current practice in peer-reviewed research journals as ``the null ritual" that involves three steps:

\begin{enumerate}
    \item Define a null hypothesis where the researcher is testing that there is no mean difference. Do not specify any predictions or alternative hypotheses.
    \item Use a \textit{p}-value of .05 for rejecting the null hypothesis and report your \textit{p}-value using a range (i.e. \textit{p} \textless .05, \textit{p} \textless .01, or \textit{p} \textless .001).
    \item Always perform this procedure.
\end{enumerate}

\noindent In 1996 the American Psychological Association (APA) brought the debate regarding the use of significance testing to the forefront by entertaining a ban in the journals it publishes \cite{Shrout1997,Hunter1997,Harris1997,Abelson1997,Scarr1997,Estes1997}. Although a ban was not instituted, APA now recommends the reporting of exact \textit{p}-values, confidence intervals, and effect sizes. 

What is the issue with \textit{p}-values? First, the practice of significance testing as represented in the social sciences for nearly a century reduces research questions to a dichotomous outcome. Rarely can a study be reduced to a simple yes/no answer, especially in the social sciences. Moreover, the use of \textit{p} \textless .05 is entirely arbitrary and the difference between significant and non-significant results is itself not significant \cite{GelmanStern2006}. However, perhaps more damning is the likelihood of committing Type II errors \cite{Bakan1966,Carver1978,Cohen1994,HenkelMorrison1970,Rozeboom1960,Schmidt1996}. A study by \citeA{SedlmeierGigerener1989} that examined all articles published in 1984 in the \textit{Journal of Abnormal Psychology} found that the error rate was 60\%. That is to say that the researchers would have done better to flip a coin!

Lastly, given the relationship between \textit{p}-values and sample size, it would be expected that with \textit{n} \textgreater 100,000, as in this study, most differences would be statistically significant. Even in one-to-one matched analysis where $n \approx 3,000$, one would expect \textit{p} \textless 0.05 for even small differences. For example, the formula for calculating \textit{t} for a dependent sample paired \textit{t}-test is:

\begin{equation}
t = \frac{{\bar{X}_D} - \mu_0}{S_D / \sqrt{n}}
\end{equation}

\noindent Where $X_D$ is the mean difference, $\mu_0$ is non-zero for testing differences other than zero, $S_D$ is the standard deviation of the differences, and $n$ is the sample size. Using the approximate sample standard deviation of 40 from grade 8 reading results, a mean difference 2 (representing a small effect size of 0.06 by most standards), and $n = 3,000$, results in $t = 2.738$ and $p = 0.003$. For the one-to-one paired analysis with a very small effect size the power estimate \cite{Cohen1988} is 0.64. However, the one-to-two matched analysis increases the power to a very acceptable 0.90. The result of this exercise is to demonstrate that relying on \textit{p}-values to make decisions is a \textit{fool's errand}. Instead, \citeA{Scarr1997} suggestion that a ``better uses of statistics would focus on the magnitude of effects and error estimates" (p. 17) is appropriate.

The use of graphics have made substantial advancements in the twentieth century, with seminal works by Tukey \cite{Cleveland1988}, \citeA{Tufte2001}, \citeauthor{Cleveland1993} (\citeyearNP{Cleveland1993,Cleveland1994}; see also \citeNP{ClevelandBecker1991}), and \citeA{Chambers1983}. The implementations in this dissertation are based upon \citeauthor{Wilkinson2005}'s \textit{grammar of graphics} as implemented in R using the \texttt{ggplot2} package \cite{Wickham2009}. Wherever possible, confidence intervals are used to show the magnitude of the differences \cite{Cumming2012,Estes1997a}. 

Although the use of graphics are frequently taught in statistics courses, they are often omitted from journal publications \cite{GelmanPasaricaDodhia2002} and relegated to diagnostic purposes \cite{Gelman2011}. The graphics presented here provide important insights into the nature and magnitude of the differences between charter and traditional public schools. The multilevel assessment plots (see Figure \ref{fig:g4math:circpsa} and \hyperref[appendixH]{Appendix H}) show the distribution scores (charter, traditional public, and differences) across multiple dimensions simultaneously. Although a table could show that the differences are small within states with few exceptions. But what would be lost is that the range of scores across states for charter and traditional public school students separately is relatively wide. Similarly, for the multilevel PSA difference plots (see Figure \ref{fig:g4math-mlpsa-ctree-diff} and \hyperref[appendixH]{Appendix H}) the graphic provides immediate evidence of the nature of the differences. Also, by providing confidence intervals, the results can be expressed vis-\`{a}-vis the graphic similar to the traditional \textit{p}-value in a summary table.

\subsection{Limitations}

This study, and many like it, only examined a small subset of students’ educational experiences. That is, NAEP, as well as CREDO \cite{credo,credo2013}, only examine student performance in math and reading. This leaves out all other, and arguably equally important, subjects. These studies are not alone in overemphasizing these subjects. The Common Core State Standards which are being implemented in the majority of states and are a cornerstone of the \textit{Race to the Top} initiative, currently only define standards and curriculum for mathematics and English Language Arts. As a consequence, the results of this study are limited to reading and math: They are silent in terms of charter school performance in writing, science, physical education, and the arts.

Lastly, this study considered charter schools as an entire, single class of schools. I believe it is important to explore the effectiveness of charter schools as compared to traditional public schools, especially given that in some cities (e.g. New Orleans and Philadelphia) the majority of students attend charter schools, and the Department of Education is currently emphasizing charter school expansion as part of the \textit{Race to the Top} initiative. However, this study, and other national studies like it \cite<e.g.>{BraunJenkinsGrigg2006,credo}, does not explicitly account for the large variability in type and quality of individual charter schools. This study does provide some evidence that there is wide variability in charter school performance on reading and math, mostly as a feature of the visualizations. Consider the multilevel PSA difference plots (e.g. Figure \ref{fig:g4math-mlpsa-ctree-diff}) where the grey points represent the difference for each stratum (i.e. students with similar covariate profiles). The spread of these points is quite large and there are some stratum with much larger gains than the overall aggregate. This suggests that, for some classes of students, charter schools provide much larger gains over their traditional school student counterparts. This should be expected, given \citeauthor{Budde1988}'s \citeyear{Budde1988} original vision and model for charter schools. That is, charter schools provide an opportunity for teachers, administrators, parents, and communities to experiment with alternative school models. Future studies should consider the varying types of charter schools (e.g. for-profit, non-profit, KIPP, online) when making comparisons with traditional public schools. 

\subsection{Conclusion}

In summary, the results of this study are consistent with the wide body of research on charter schools. Namely, some charter schools perform better than their traditional public school counterpart on NAEP reading and math tests, while others perform worse. However, in aggregate, the average difference is nonexistent or very small. Ray Budde \citeyear{Budde1988} originally envisioned charter schools as a way for teachers, administrators, parents, and communities to experiment with the goal of finding better ways to teach students. But with \textit{No Child Left Behind} and \textit{Race to the Top}, among other initiatives from private for-profit and not-for-profit organizations, charter schools are often offered as a wholesale replacement for traditional public schools. That is, charter schowols were originally envisioned as experimental schools but more recently have been presented as schools of choice (to traditional public schools). The results of this study, along with the other national studies examining the differences between charter and traditional public schools \cite{credo,credo2013,BraunJenkinsGrigg2006}, suggest that charter schools do not provide, in aggregate, substantial benefit on NAEP reading and math scores over their traditional public school counterparts.


%==================== REFERENCES ====================================================================
%\clearpage
\bibliographystyle{apacite}
\bibliography{Bibliography}

\input{Bryer.Appendicies}

\end{document}
